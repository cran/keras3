<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Multi-GPU distributed training with JAX</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Multi-GPU distributed training with
JAX</h1>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>There are generally two ways to distribute computation across
multiple devices:</p>
<p><strong>Data parallelism</strong>, where a single model gets
replicated on multiple devices or multiple machines. Each of them
processes different batches of data, then they merge their results.
There exist many variants of this setup, that differ in how the
different model replicas merge results, in whether they stay in sync at
every batch or whether they are more loosely coupled, etc.</p>
<p><strong>Model parallelism</strong>, where different parts of a single
model run on different devices, processing a single batch of data
together. This works best with models that have a naturally-parallel
architecture, such as models that feature multiple branches.</p>
<p>This guide focuses on data parallelism, in particular
<strong>synchronous data parallelism</strong>, where the different
replicas of the model stay in sync after each batch they process.
Synchronicity keeps the model convergence behavior identical to what you
would see for single-device training.</p>
<p>Specifically, this guide teaches you how to use
<code>jax.sharding</code> APIs to train Keras models, with minimal
changes to your code, on multiple GPUs or TPUS (typically 2 to 16)
installed on a single machine (single host, multi-device training). This
is the most common setup for researchers and small-scale industry
workflows.</p>
</div>
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>Let’s start by defining the function that creates the model that we
will train, and the function that creates the dataset we will train on
(MNIST in this case).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>os.environ[<span class="st">&quot;KERAS_BACKEND&quot;</span>] <span class="op">=</span> <span class="st">&quot;jax&quot;</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">from</span> jax.experimental <span class="im">import</span> mesh_utils</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">from</span> jax.sharding <span class="im">import</span> Mesh</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="im">from</span> jax.sharding <span class="im">import</span> NamedSharding</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="im">from</span> jax.sharding <span class="im">import</span> PartitionSpec <span class="im">as</span> P</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="kw">def</span> get_model():</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>    <span class="co"># Make a simple convnet with batch normalization and dropout.</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Rescaling(<span class="fl">1.0</span> <span class="op">/</span> <span class="fl">255.0</span>)(inputs)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">12</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">&quot;same&quot;</span>, use_bias<span class="op">=</span><span class="va">False</span></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>    )(x)</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">False</span>, center<span class="op">=</span><span class="va">True</span>)(x)</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.ReLU()(x)</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(</span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">24</span>,</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a>        use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a>        strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a>    )(x)</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">False</span>, center<span class="op">=</span><span class="va">True</span>)(x)</span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.ReLU()(x)</span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(</span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">&quot;same&quot;</span>,</span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a>        strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a>        name<span class="op">=</span><span class="st">&quot;large_k&quot;</span>,</span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a>    )(x)</span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">False</span>, center<span class="op">=</span><span class="va">True</span>)(x)</span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.ReLU()(x)</span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.GlobalAveragePooling2D()(x)</span>
<span id="cb1-43"><a href="#cb1-43" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>)(x)</span>
<span id="cb1-44"><a href="#cb1-44" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb1-45"><a href="#cb1-45" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb1-46"><a href="#cb1-46" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb1-47"><a href="#cb1-47" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb1-48"><a href="#cb1-48" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" tabindex="-1"></a><span class="kw">def</span> get_datasets():</span>
<span id="cb1-51"><a href="#cb1-51" tabindex="-1"></a>    <span class="co"># Load the data and split it between train and test sets</span></span>
<span id="cb1-52"><a href="#cb1-52" tabindex="-1"></a>    (x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.mnist.load_data()</span>
<span id="cb1-53"><a href="#cb1-53" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" tabindex="-1"></a>    <span class="co"># Scale images to the [0, 1] range</span></span>
<span id="cb1-55"><a href="#cb1-55" tabindex="-1"></a>    x_train <span class="op">=</span> x_train.astype(<span class="st">&quot;float32&quot;</span>)</span>
<span id="cb1-56"><a href="#cb1-56" tabindex="-1"></a>    x_test <span class="op">=</span> x_test.astype(<span class="st">&quot;float32&quot;</span>)</span>
<span id="cb1-57"><a href="#cb1-57" tabindex="-1"></a>    <span class="co"># Make sure images have shape (28, 28, 1)</span></span>
<span id="cb1-58"><a href="#cb1-58" tabindex="-1"></a>    x_train <span class="op">=</span> np.expand_dims(x_train, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-59"><a href="#cb1-59" tabindex="-1"></a>    x_test <span class="op">=</span> np.expand_dims(x_test, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-60"><a href="#cb1-60" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;x_train shape:&quot;</span>, x_train.shape)</span>
<span id="cb1-61"><a href="#cb1-61" tabindex="-1"></a>    <span class="bu">print</span>(x_train.shape[<span class="dv">0</span>], <span class="st">&quot;train samples&quot;</span>)</span>
<span id="cb1-62"><a href="#cb1-62" tabindex="-1"></a>    <span class="bu">print</span>(x_test.shape[<span class="dv">0</span>], <span class="st">&quot;test samples&quot;</span>)</span>
<span id="cb1-63"><a href="#cb1-63" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" tabindex="-1"></a>    <span class="co"># Create TF Datasets</span></span>
<span id="cb1-65"><a href="#cb1-65" tabindex="-1"></a>    train_data <span class="op">=</span> tf.data.Dataset.from_tensor_slices((x_train, y_train))</span>
<span id="cb1-66"><a href="#cb1-66" tabindex="-1"></a>    eval_data <span class="op">=</span> tf.data.Dataset.from_tensor_slices((x_test, y_test))</span>
<span id="cb1-67"><a href="#cb1-67" tabindex="-1"></a>    <span class="cf">return</span> train_data, eval_data</span></code></pre></div>
</div>
<div id="single-host-multi-device-synchronous-training" class="section level2">
<h2>Single-host, multi-device synchronous training</h2>
<p>In this setup, you have one machine with several GPUs or TPUs on it
(typically 2 to 16). Each device will run a copy of your model (called a
<strong>replica</strong>). For simplicity, in what follows, we’ll assume
we’re dealing with 8 GPUs, at no loss of generality.</p>
<p><strong>How it works</strong></p>
<p>At each step of training:</p>
<ul>
<li>The current batch of data (called <strong>global batch</strong>) is
split into 8 different sub-batches (called <strong>local
batches</strong>). For instance, if the global batch has 512 samples,
each of the 8 local batches will have 64 samples.</li>
<li>Each of the 8 replicas independently processes a local batch: they
run a forward pass, then a backward pass, outputting the gradient of the
weights with respect to the loss of the model on the local batch.</li>
<li>The weight updates originating from local gradients are efficiently
merged across the 8 replicas. Because this is done at the end of every
step, the replicas always stay in sync.</li>
</ul>
<p>In practice, the process of synchronously updating the weights of the
model replicas is handled at the level of each individual weight
variable. This is done through a using a
<code>jax.sharding.NamedSharding</code> that is configured to replicate
the variables.</p>
<p><strong>How to use it</strong></p>
<p>To do single-host, multi-device synchronous training with a Keras
model, you would use the <code>jax.sharding</code> features. Here’s how
it works:</p>
<ul>
<li>We first create a device mesh using
<code>mesh_utils.create_device_mesh</code>.</li>
<li>We use <code>jax.sharding.Mesh</code>,
<code>jax.sharding.NamedSharding</code> and
<code>jax.sharding.PartitionSpec</code> to define how to partition JAX
arrays.
<ul>
<li>We specify that we want to replicate the model and optimizer
variables across all devices by using a spec with no axis.</li>
<li>We specify that we want to shard the data across devices by using a
spec that splits along the batch dimension.</li>
</ul></li>
<li>We use <code>jax.device_put</code> to replicate the model and
optimizer variables across devices. This happens once at the
beginning.</li>
<li>In the training loop, for each batch that we process, we use
<code>jax.device_put</code> to split the batch across devices before
invoking the train step.</li>
</ul>
<p>Here’s the flow, where each step is split into its own utility
function:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Config</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>train_data, eval_data <span class="op">=</span> get_datasets()</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>train_data <span class="op">=</span> train_data.batch(batch_size, drop_remainder<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>model <span class="op">=</span> get_model()</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Adam(<span class="fl">1e-3</span>)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co"># Initialize all state with .build()</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>(one_batch, one_batch_labels) <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_data))</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>model.build(one_batch)</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>optimizer.build(model.trainable_variables)</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a><span class="co"># This is the loss function that will be differentiated.</span></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a><span class="co"># Keras provides a pure functional forward pass: model.stateless_call</span></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a><span class="kw">def</span> compute_loss(trainable_variables, non_trainable_variables, x, y):</span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>    y_pred, updated_non_trainable_variables <span class="op">=</span> model.stateless_call(</span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>        trainable_variables, non_trainable_variables, x</span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>    )</span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a>    loss_value <span class="op">=</span> loss(y, y_pred)</span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a>    <span class="cf">return</span> loss_value, updated_non_trainable_variables</span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a><span class="co"># Function to compute gradients</span></span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a>compute_gradients <span class="op">=</span> jax.value_and_grad(compute_loss, has_aux<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" tabindex="-1"></a><span class="co"># Training step, Keras provides a pure functional optimizer.stateless_apply</span></span>
<span id="cb2-33"><a href="#cb2-33" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb2-34"><a href="#cb2-34" tabindex="-1"></a><span class="kw">def</span> train_step(train_state, x, y):</span>
<span id="cb2-35"><a href="#cb2-35" tabindex="-1"></a>    (</span>
<span id="cb2-36"><a href="#cb2-36" tabindex="-1"></a>        trainable_variables,</span>
<span id="cb2-37"><a href="#cb2-37" tabindex="-1"></a>        non_trainable_variables,</span>
<span id="cb2-38"><a href="#cb2-38" tabindex="-1"></a>        optimizer_variables,</span>
<span id="cb2-39"><a href="#cb2-39" tabindex="-1"></a>    ) <span class="op">=</span> train_state</span>
<span id="cb2-40"><a href="#cb2-40" tabindex="-1"></a>    (loss_value, non_trainable_variables), grads <span class="op">=</span> compute_gradients(</span>
<span id="cb2-41"><a href="#cb2-41" tabindex="-1"></a>        trainable_variables, non_trainable_variables, x, y</span>
<span id="cb2-42"><a href="#cb2-42" tabindex="-1"></a>    )</span>
<span id="cb2-43"><a href="#cb2-43" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" tabindex="-1"></a>    trainable_variables, optimizer_variables <span class="op">=</span> optimizer.stateless_apply(</span>
<span id="cb2-45"><a href="#cb2-45" tabindex="-1"></a>        optimizer_variables, grads, trainable_variables</span>
<span id="cb2-46"><a href="#cb2-46" tabindex="-1"></a>    )</span>
<span id="cb2-47"><a href="#cb2-47" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" tabindex="-1"></a>    <span class="cf">return</span> loss_value, (</span>
<span id="cb2-49"><a href="#cb2-49" tabindex="-1"></a>        trainable_variables,</span>
<span id="cb2-50"><a href="#cb2-50" tabindex="-1"></a>        non_trainable_variables,</span>
<span id="cb2-51"><a href="#cb2-51" tabindex="-1"></a>        optimizer_variables,</span>
<span id="cb2-52"><a href="#cb2-52" tabindex="-1"></a>    )</span>
<span id="cb2-53"><a href="#cb2-53" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" tabindex="-1"></a><span class="co"># Replicate the model and optimizer variable on all devices</span></span>
<span id="cb2-56"><a href="#cb2-56" tabindex="-1"></a><span class="kw">def</span> get_replicated_train_state(devices):</span>
<span id="cb2-57"><a href="#cb2-57" tabindex="-1"></a>    <span class="co"># All variables will be replicated on all devices</span></span>
<span id="cb2-58"><a href="#cb2-58" tabindex="-1"></a>    var_mesh <span class="op">=</span> Mesh(devices, axis_names<span class="op">=</span>(<span class="st">&quot;_&quot;</span>))</span>
<span id="cb2-59"><a href="#cb2-59" tabindex="-1"></a>    <span class="co"># In NamedSharding, axes not mentioned are replicated (all axes here)</span></span>
<span id="cb2-60"><a href="#cb2-60" tabindex="-1"></a>    var_replication <span class="op">=</span> NamedSharding(var_mesh, P())</span>
<span id="cb2-61"><a href="#cb2-61" tabindex="-1"></a></span>
<span id="cb2-62"><a href="#cb2-62" tabindex="-1"></a>    <span class="co"># Apply the distribution settings to the model variables</span></span>
<span id="cb2-63"><a href="#cb2-63" tabindex="-1"></a>    trainable_variables <span class="op">=</span> jax.device_put(</span>
<span id="cb2-64"><a href="#cb2-64" tabindex="-1"></a>        model.trainable_variables, var_replication</span>
<span id="cb2-65"><a href="#cb2-65" tabindex="-1"></a>    )</span>
<span id="cb2-66"><a href="#cb2-66" tabindex="-1"></a>    non_trainable_variables <span class="op">=</span> jax.device_put(</span>
<span id="cb2-67"><a href="#cb2-67" tabindex="-1"></a>        model.non_trainable_variables, var_replication</span>
<span id="cb2-68"><a href="#cb2-68" tabindex="-1"></a>    )</span>
<span id="cb2-69"><a href="#cb2-69" tabindex="-1"></a>    optimizer_variables <span class="op">=</span> jax.device_put(optimizer.variables, var_replication)</span>
<span id="cb2-70"><a href="#cb2-70" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" tabindex="-1"></a>    <span class="co"># Combine all state in a tuple</span></span>
<span id="cb2-72"><a href="#cb2-72" tabindex="-1"></a>    <span class="cf">return</span> (trainable_variables, non_trainable_variables, optimizer_variables)</span>
<span id="cb2-73"><a href="#cb2-73" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" tabindex="-1"></a>num_devices <span class="op">=</span> <span class="bu">len</span>(jax.local_devices())</span>
<span id="cb2-76"><a href="#cb2-76" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Running on </span><span class="sc">{</span>num_devices<span class="sc">}</span><span class="ss"> devices: </span><span class="sc">{</span>jax<span class="sc">.</span>local_devices()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb2-77"><a href="#cb2-77" tabindex="-1"></a>devices <span class="op">=</span> mesh_utils.create_device_mesh((num_devices,))</span>
<span id="cb2-78"><a href="#cb2-78" tabindex="-1"></a></span>
<span id="cb2-79"><a href="#cb2-79" tabindex="-1"></a><span class="co"># Data will be split along the batch axis</span></span>
<span id="cb2-80"><a href="#cb2-80" tabindex="-1"></a>data_mesh <span class="op">=</span> Mesh(devices, axis_names<span class="op">=</span>(<span class="st">&quot;batch&quot;</span>,))  <span class="co"># naming axes of the mesh</span></span>
<span id="cb2-81"><a href="#cb2-81" tabindex="-1"></a>data_sharding <span class="op">=</span> NamedSharding(</span>
<span id="cb2-82"><a href="#cb2-82" tabindex="-1"></a>    data_mesh,</span>
<span id="cb2-83"><a href="#cb2-83" tabindex="-1"></a>    P(</span>
<span id="cb2-84"><a href="#cb2-84" tabindex="-1"></a>        <span class="st">&quot;batch&quot;</span>,</span>
<span id="cb2-85"><a href="#cb2-85" tabindex="-1"></a>    ),</span>
<span id="cb2-86"><a href="#cb2-86" tabindex="-1"></a>)  <span class="co"># naming axes of the sharded partition</span></span>
<span id="cb2-87"><a href="#cb2-87" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" tabindex="-1"></a><span class="co"># Display data sharding</span></span>
<span id="cb2-89"><a href="#cb2-89" tabindex="-1"></a>x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_data))</span>
<span id="cb2-90"><a href="#cb2-90" tabindex="-1"></a>sharded_x <span class="op">=</span> jax.device_put(x.numpy(), data_sharding)</span>
<span id="cb2-91"><a href="#cb2-91" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Data sharding&quot;</span>)</span>
<span id="cb2-92"><a href="#cb2-92" tabindex="-1"></a>jax.debug.visualize_array_sharding(jax.numpy.reshape(sharded_x, [<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>]))</span>
<span id="cb2-93"><a href="#cb2-93" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" tabindex="-1"></a>train_state <span class="op">=</span> get_replicated_train_state(devices)</span>
<span id="cb2-95"><a href="#cb2-95" tabindex="-1"></a></span>
<span id="cb2-96"><a href="#cb2-96" tabindex="-1"></a><span class="co"># Custom training loop</span></span>
<span id="cb2-97"><a href="#cb2-97" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb2-98"><a href="#cb2-98" tabindex="-1"></a>    data_iter <span class="op">=</span> <span class="bu">iter</span>(train_data)</span>
<span id="cb2-99"><a href="#cb2-99" tabindex="-1"></a>    <span class="cf">for</span> data <span class="kw">in</span> data_iter:</span>
<span id="cb2-100"><a href="#cb2-100" tabindex="-1"></a>        x, y <span class="op">=</span> data</span>
<span id="cb2-101"><a href="#cb2-101" tabindex="-1"></a>        sharded_x <span class="op">=</span> jax.device_put(x.numpy(), data_sharding)</span>
<span id="cb2-102"><a href="#cb2-102" tabindex="-1"></a>        loss_value, train_state <span class="op">=</span> train_step(train_state, sharded_x, y.numpy())</span>
<span id="cb2-103"><a href="#cb2-103" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Epoch&quot;</span>, epoch, <span class="st">&quot;loss:&quot;</span>, loss_value)</span>
<span id="cb2-104"><a href="#cb2-104" tabindex="-1"></a></span>
<span id="cb2-105"><a href="#cb2-105" tabindex="-1"></a><span class="co"># Post-processing model state update to write them back into the model</span></span>
<span id="cb2-106"><a href="#cb2-106" tabindex="-1"></a>trainable_variables, non_trainable_variables, optimizer_variables <span class="op">=</span> train_state</span>
<span id="cb2-107"><a href="#cb2-107" tabindex="-1"></a><span class="cf">for</span> variable, value <span class="kw">in</span> <span class="bu">zip</span>(model.trainable_variables, trainable_variables):</span>
<span id="cb2-108"><a href="#cb2-108" tabindex="-1"></a>    variable.assign(value)</span>
<span id="cb2-109"><a href="#cb2-109" tabindex="-1"></a><span class="cf">for</span> variable, value <span class="kw">in</span> <span class="bu">zip</span>(</span>
<span id="cb2-110"><a href="#cb2-110" tabindex="-1"></a>    model.non_trainable_variables, non_trainable_variables</span>
<span id="cb2-111"><a href="#cb2-111" tabindex="-1"></a>):</span>
<span id="cb2-112"><a href="#cb2-112" tabindex="-1"></a>    variable.assign(value)</span></code></pre></div>
<p>That’s it!</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
