<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Making new layers and models via subclassing</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Making new layers and models via
subclassing</h1>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This guide will cover everything you need to know to build your own
subclassed layers and models. In particular, you’ll learn about the
following features:</p>
<ul>
<li>The <code>Layer</code> class</li>
<li>The <code>add_weight()</code> method</li>
<li>Trainable and non-trainable weights</li>
<li>The <code>build()</code> method</li>
<li>Making sure your layers can be used with any backend</li>
<li>The <code>add_loss()</code> method</li>
<li>The <code>training</code> argument in <code>call()</code></li>
<li>The <code>mask</code> argument in <code>call()</code></li>
<li>Making sure your layers can be serialized</li>
</ul>
<p>Let’s dive in.</p>
</div>
<div id="setup" class="section level2">
<h2>Setup</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(keras3)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(tensorflow, <span class="at">exclude =</span> <span class="fu">c</span>(<span class="st">&quot;set_random_seed&quot;</span>, <span class="st">&quot;shape&quot;</span>))</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">library</span>(tfdatasets, <span class="at">exclude =</span> <span class="st">&quot;shape&quot;</span>)</span></code></pre></div>
</div>
<div id="the-layer-class-the-combination-of-state-weights-and-some-computation" class="section level2">
<h2>The <code>Layer</code> class: the combination of state (weights) and
some computation</h2>
<p>One of the central abstractions in Keras is the <code>Layer</code>
class. A layer encapsulates both a state (the layer’s “weights”) and a
transformation from inputs to outputs (a “call”, the layer’s forward
pass).</p>
<p>Here’s a densely-connected layer. It has two state variables: the
variables <code>w</code> and <code>b</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>layer_linear <span class="ot">&lt;-</span> <span class="fu">Layer</span>(<span class="st">&quot;Linear&quot;</span>,</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">input_dim =</span> <span class="dv">32</span>, ...) {</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>    self<span class="sc">$</span>w <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>      <span class="at">shape =</span> <span class="fu">shape</span>(input_dim, units),</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>      <span class="at">initializer =</span> <span class="st">&quot;random_normal&quot;</span>,</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>      <span class="at">trainable =</span> <span class="cn">TRUE</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>    )</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>    self<span class="sc">$</span>b <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>      <span class="at">shape =</span> <span class="fu">shape</span>(units),</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>      <span class="at">initializer =</span> <span class="st">&quot;zeros&quot;</span>,</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>      <span class="at">trainable =</span> <span class="cn">TRUE</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>    )</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>  },</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>    <span class="fu">op_matmul</span>(inputs, self<span class="sc">$</span>w) <span class="sc">+</span> self<span class="sc">$</span>b</span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>  }</span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>)</span></code></pre></div>
<p>You would use a layer by calling it on some tensor input(s), much
like an R function.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">op_ones</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>linear_layer <span class="ot">&lt;-</span> <span class="fu">layer_linear</span>(<span class="at">units =</span> <span class="dv">4</span>, <span class="at">input_dim =</span> <span class="dv">2</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">linear_layer</span>(x)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="fu">print</span>(y)</span></code></pre></div>
<pre><code>## tf.Tensor(
## [[0.02153057 0.15450525 0.0205495  0.04493225]
##  [0.02153057 0.15450525 0.0205495  0.04493225]], shape=(2, 4), dtype=float32)</code></pre>
<p>Note that the weights <code>w</code> and <code>b</code> are
automatically tracked by the layer upon being set as layer
attributes:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>linear_layer<span class="sc">$</span>weights</span></code></pre></div>
<pre><code>## [[1]]
## &lt;Variable path=linear/variable, shape=(2, 4), dtype=float32, value=[[-0.06251299  0.05335509  0.01485647 -0.00985784]
##  [ 0.08404355  0.10115016  0.00569303  0.05479009]]&gt;
##
## [[2]]
## &lt;Variable path=linear/variable_1, shape=(4), dtype=float32, value=[0. 0. 0. 0.]&gt;</code></pre>
</div>
<div id="layers-can-have-non-trainable-weights" class="section level2">
<h2>Layers can have non-trainable weights</h2>
<p>Besides trainable weights, you can add non-trainable weights to a
layer as well. Such weights are meant not to be taken into account
during backpropagation, when you are training the layer.</p>
<p>Here’s how to add and use a non-trainable weight:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>layer_compute_sum <span class="ot">&lt;-</span> <span class="fu">Layer</span>(</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>  <span class="st">&quot;ComputeSum&quot;</span>,</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(input_dim) {</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>()</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    self<span class="sc">$</span>total <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>      <span class="at">initializer =</span> <span class="st">&quot;zeros&quot;</span>,</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>      <span class="at">shape =</span> <span class="fu">shape</span>(input_dim),</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>      <span class="at">trainable =</span> <span class="cn">FALSE</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>    )</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>  },</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>    self<span class="sc">$</span>total<span class="sc">$</span><span class="fu">assign_add</span>(<span class="fu">op_sum</span>(inputs, <span class="at">axis =</span> <span class="dv">1</span>))</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>    self<span class="sc">$</span>total</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>  }</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>)</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">op_ones</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>my_sum <span class="ot">&lt;-</span> <span class="fu">layer_compute_sum</span>(<span class="at">input_dim =</span> <span class="dv">2</span>)</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">my_sum</span>(x)</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">as.array</span>(y))</span></code></pre></div>
<pre><code>## [1] 2 2</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">my_sum</span>(x)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">as.array</span>(y))</span></code></pre></div>
<pre><code>## [1] 4 4</code></pre>
<p>It’s part of <code>layer$weights</code>, but it gets categorized as a
non-trainable weight:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;weights:&quot;</span>, <span class="fu">length</span>(my_sum<span class="sc">$</span>weights))</span></code></pre></div>
<pre><code>## weights: 1</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;non-trainable weights:&quot;</span>, <span class="fu">length</span>(my_sum<span class="sc">$</span>non_trainable_weights))</span></code></pre></div>
<pre><code>## non-trainable weights: 1</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="co"># It&#39;s not included in the trainable weights:</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;trainable_weights:&quot;</span>, <span class="fu">length</span>(my_sum<span class="sc">$</span>trainable_weights))</span></code></pre></div>
<pre><code>## trainable_weights: 0</code></pre>
</div>
<div id="best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known" class="section level2">
<h2>Best practice: deferring weight creation until the shape of the
inputs is known</h2>
<p>Our <code>Linear</code> layer above took an <code>input_dim</code>
argument that was used to compute the shape of the weights
<code>w</code> and <code>b</code> in <code>initialize()</code>:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>layer_linear <span class="ot">&lt;-</span> <span class="fu">Layer</span>(<span class="st">&quot;Linear&quot;</span>,</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">input_dim =</span> <span class="dv">32</span>, ...) {</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>    self<span class="sc">$</span>w <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>      <span class="at">shape =</span> <span class="fu">shape</span>(input_dim, units),</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>      <span class="at">initializer =</span> <span class="st">&quot;random_normal&quot;</span>,</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>      <span class="at">trainable =</span> <span class="cn">TRUE</span></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>    )</span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>    self<span class="sc">$</span>b <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>      <span class="at">shape =</span> <span class="fu">shape</span>(units),</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>      <span class="at">initializer =</span> <span class="st">&quot;zeros&quot;</span>,</span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>      <span class="at">trainable =</span> <span class="cn">TRUE</span></span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a>    )</span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a>  },</span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb17-18"><a href="#cb17-18" tabindex="-1"></a>    <span class="fu">op_matmul</span>(inputs, self<span class="sc">$</span>w) <span class="sc">+</span> self<span class="sc">$</span>b</span>
<span id="cb17-19"><a href="#cb17-19" tabindex="-1"></a>  }</span>
<span id="cb17-20"><a href="#cb17-20" tabindex="-1"></a>)</span></code></pre></div>
<p>In many cases, you may not know in advance the size of your inputs,
and you would like to lazily create weights when that value becomes
known, some time after instantiating the layer.</p>
<p>In the Keras API, we recommend creating layer weights in the
<code>build(self, inputs_shape)</code> method of your layer. Like
this:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>layer_linear <span class="ot">&lt;-</span> <span class="fu">Layer</span>(</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>  <span class="st">&quot;Linear&quot;</span>,</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(<span class="at">units =</span> <span class="dv">32</span>, ...) {</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>    self<span class="sc">$</span>units <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(units)</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>  },</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>  <span class="at">build =</span> <span class="cf">function</span>(input_shape) {</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>    self<span class="sc">$</span>w <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(</span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a>      <span class="at">shape =</span> <span class="fu">shape</span>(<span class="fu">tail</span>(input_shape, <span class="dv">1</span>), self<span class="sc">$</span>units),</span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a>      <span class="at">initializer =</span> <span class="st">&quot;random_normal&quot;</span>,</span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>      <span class="at">trainable =</span> <span class="cn">TRUE</span></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>    )</span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a>    self<span class="sc">$</span>b <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(</span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a>      <span class="at">shape =</span> <span class="fu">shape</span>(self<span class="sc">$</span>units),</span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a>      <span class="at">initializer =</span> <span class="st">&quot;zeros&quot;</span>,</span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>      <span class="at">trainable =</span> <span class="cn">TRUE</span></span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>    )</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a>  },</span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a>    <span class="fu">op_matmul</span>(inputs, self<span class="sc">$</span>w) <span class="sc">+</span> self<span class="sc">$</span>b</span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a>  }</span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a>)</span></code></pre></div>
<p>The <code>call()</code> method of your layer will automatically run
build the first time it is called. You now have a layer that’s lazy and
thus easier to use:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="co"># At instantiation, we don&#39;t know on what inputs this is going to get called</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>linear_layer <span class="ot">&lt;-</span> <span class="fu">layer_linear</span>(<span class="at">units =</span> <span class="dv">32</span>)</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a><span class="co"># The layer&#39;s weights are created dynamically the first time the layer is called</span></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">linear_layer</span>(x)</span></code></pre></div>
<p>Implementing <code>build()</code> separately as shown above nicely
separates creating weights only once from using weights in every
call.</p>
</div>
<div id="layers-are-recursively-composable" class="section level2">
<h2>Layers are recursively composable</h2>
<p>If you assign a Layer instance as an attribute of another Layer, the
outer layer will start tracking the weights created by the inner
layer.</p>
<p>We recommend creating such sublayers in the <code>initialize()</code>
method and leave it to the first <code>call()</code> to trigger building
their weights.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>MLPBlock <span class="ot">&lt;-</span> <span class="fu">Layer</span>(</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>  <span class="st">&quot;MLPBlock&quot;</span>,</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>() {</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>()</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>    self<span class="sc">$</span>linear_1 <span class="ot">&lt;-</span> <span class="fu">layer_linear</span>(<span class="at">units =</span> <span class="dv">32</span>)</span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a>    self<span class="sc">$</span>linear_2 <span class="ot">&lt;-</span> <span class="fu">layer_linear</span>(<span class="at">units =</span> <span class="dv">32</span>)</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a>    self<span class="sc">$</span>linear_3 <span class="ot">&lt;-</span> <span class="fu">layer_linear</span>(<span class="at">units =</span> <span class="dv">1</span>)</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a>  },</span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a>    inputs <span class="sc">|&gt;</span></span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">linear_1</span>() <span class="sc">|&gt;</span></span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a>      <span class="fu">activation_relu</span>() <span class="sc">|&gt;</span></span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">linear_2</span>() <span class="sc">|&gt;</span></span>
<span id="cb20-14"><a href="#cb20-14" tabindex="-1"></a>      <span class="fu">activation_relu</span>() <span class="sc">|&gt;</span></span>
<span id="cb20-15"><a href="#cb20-15" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">linear_3</span>()</span>
<span id="cb20-16"><a href="#cb20-16" tabindex="-1"></a>  }</span>
<span id="cb20-17"><a href="#cb20-17" tabindex="-1"></a>)</span>
<span id="cb20-18"><a href="#cb20-18" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" tabindex="-1"></a>mlp <span class="ot">&lt;-</span> <span class="fu">MLPBlock</span>()</span>
<span id="cb20-20"><a href="#cb20-20" tabindex="-1"></a><span class="co"># The first call to the `mlp` will create the weights</span></span>
<span id="cb20-21"><a href="#cb20-21" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">mlp</span>(<span class="fu">op_ones</span>(<span class="at">shape =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">64</span>)))</span>
<span id="cb20-22"><a href="#cb20-22" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;weights:&quot;</span>, <span class="fu">length</span>(mlp<span class="sc">$</span>weights), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## weights: 6</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;trainable weights:&quot;</span>, <span class="fu">length</span>(mlp<span class="sc">$</span>trainable_weights), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## trainable weights: 6</code></pre>
</div>
<div id="backend-agnostic-layers-and-backend-specific-layers" class="section level2">
<h2>Backend-agnostic layers and backend-specific layers</h2>
<p>As long as a layer only uses APIs from the <code>ops</code> namespace
(ie. using functions starting with <code>op_</code>), (or other Keras
namespaces such as <code>activations_*</code>, <code>random_*</code>, or
<code>layer_*</code>), then it can be used with any backend –
TensorFlow, JAX, or PyTorch.</p>
<p>All layers you’ve seen so far in this guide work with all Keras
backends.</p>
<p>The <code>ops</code> namespace gives you access to:</p>
<ul>
<li>The NumPy API, e.g. <code>op_matmul</code>, <code>op_sum</code>,
<code>op_reshape</code>, <code>op_stack</code>, etc.</li>
<li>Neural networks-specific APIs such as <code>op_softmax</code>,
<code>op_conv</code>, <code>op_binary_crossentropy</code>,
<code>op_relu</code>, etc.</li>
</ul>
<p>You can also use backend-native APIs in your layers (such as
<code>tf$nn</code> functions), but if you do this, then your layer will
only be usable with the backend in question. For instance, you could
write the following JAX-specific layer using <code>jax$numpy</code>:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="co"># keras3::install_keras(backend = c(&quot;jax&quot;))</span></span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>jax <span class="ot">&lt;-</span> reticulate<span class="sc">::</span><span class="fu">import</span>(<span class="st">&quot;jax&quot;</span>)</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>Linear <span class="ot">&lt;-</span> <span class="fu">new_layer_class</span>(</span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a>  ...</span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a>    jax<span class="sc">$</span>numpy<span class="sc">$</span><span class="fu">matmul</span>(inputs, self<span class="sc">$</span>w) <span class="sc">+</span> self<span class="sc">$</span>b</span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a>  }</span>
<span id="cb24-9"><a href="#cb24-9" tabindex="-1"></a>)</span></code></pre></div>
<p>This would be the equivalent TensorFlow-specific layer:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="fu">library</span>(tensorflow)</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>Linear <span class="ot">&lt;-</span> <span class="fu">new_layer_class</span>(</span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a>  ...</span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a>    tf<span class="sc">$</span><span class="fu">matmul</span>(inputs, self<span class="sc">$</span>w) <span class="sc">+</span> self<span class="sc">$</span>b</span>
<span id="cb25-7"><a href="#cb25-7" tabindex="-1"></a>  }</span>
<span id="cb25-8"><a href="#cb25-8" tabindex="-1"></a>)</span></code></pre></div>
<p>And this would be the equivalent PyTorch-specific layer:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>torch <span class="ot">&lt;-</span> reticulate<span class="sc">::</span><span class="fu">import</span>(<span class="st">&quot;torch&quot;</span>)</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>Linear <span class="ot">&lt;-</span> <span class="fu">new_layer_class</span>(</span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>  ...</span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a>    torch<span class="sc">$</span><span class="fu">matmul</span>(inputs, self<span class="sc">$</span>w) <span class="sc">+</span> self<span class="sc">$</span>b</span>
<span id="cb26-7"><a href="#cb26-7" tabindex="-1"></a>  }</span>
<span id="cb26-8"><a href="#cb26-8" tabindex="-1"></a>)</span></code></pre></div>
<p>Because cross-backend compatibility is a tremendously useful
property, we strongly recommend that you seek to always make your layers
backend-agnostic by leveraging only Keras APIs.</p>
</div>
<div id="the-add_loss-method" class="section level2">
<h2>The <code>add_loss()</code> method</h2>
<p>When writing the <code>call()</code> method of a layer, you can
create loss tensors that you will want to use later, when writing your
training loop. This is doable by calling
<code>self$add_loss(value)</code>:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="co"># A layer that creates an activity regularization loss</span></span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a>layer_activity_regularization <span class="ot">&lt;-</span> <span class="fu">Layer</span>(</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>  <span class="st">&quot;ActivityRegularizationLayer&quot;</span>,</span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(<span class="at">rate =</span> <span class="fl">1e-2</span>) {</span>
<span id="cb27-5"><a href="#cb27-5" tabindex="-1"></a>    self<span class="sc">$</span>rate <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(rate)</span>
<span id="cb27-6"><a href="#cb27-6" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>()</span>
<span id="cb27-7"><a href="#cb27-7" tabindex="-1"></a>  },</span>
<span id="cb27-8"><a href="#cb27-8" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb27-9"><a href="#cb27-9" tabindex="-1"></a>    self<span class="sc">$</span><span class="fu">add_loss</span>(self<span class="sc">$</span>rate <span class="sc">*</span> <span class="fu">op_mean</span>(inputs))</span>
<span id="cb27-10"><a href="#cb27-10" tabindex="-1"></a>    inputs</span>
<span id="cb27-11"><a href="#cb27-11" tabindex="-1"></a>  }</span>
<span id="cb27-12"><a href="#cb27-12" tabindex="-1"></a>)</span></code></pre></div>
<p>These losses (including those created by any inner layer) can be
retrieved via <code>layer$losses</code>. This property is reset at the
start of every <code>call</code> to the top-level layer, so that
<code>layer$losses</code> always contains the loss values created during
the last forward pass.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>layer_outer <span class="ot">&lt;-</span> <span class="fu">Layer</span>(</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>  <span class="st">&quot;OuterLayer&quot;</span>,</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>() {</span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>()</span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a>    self<span class="sc">$</span>activity_reg <span class="ot">&lt;-</span> <span class="fu">layer_activity_regularization</span>(<span class="at">rate =</span> <span class="fl">1e-2</span>)</span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a>  },</span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb28-8"><a href="#cb28-8" tabindex="-1"></a>    self<span class="sc">$</span><span class="fu">activity_reg</span>(inputs)</span>
<span id="cb28-9"><a href="#cb28-9" tabindex="-1"></a>    inputs</span>
<span id="cb28-10"><a href="#cb28-10" tabindex="-1"></a>  }</span>
<span id="cb28-11"><a href="#cb28-11" tabindex="-1"></a>)</span>
<span id="cb28-12"><a href="#cb28-12" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" tabindex="-1"></a>layer <span class="ot">&lt;-</span> <span class="fu">layer_outer</span>()</span>
<span id="cb28-14"><a href="#cb28-14" tabindex="-1"></a><span class="co"># No losses yet since the layer has never been called</span></span>
<span id="cb28-15"><a href="#cb28-15" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;losses:&quot;</span>, <span class="fu">length</span>(layer<span class="sc">$</span>losses), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## losses: 0</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">layer</span>(<span class="fu">op_zeros</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a><span class="co"># We created one loss value</span></span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;losses:&quot;</span>, <span class="fu">length</span>(layer<span class="sc">$</span>losses), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## losses: 1</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="co"># `layer$losses` gets reset at the start of each call</span></span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">layer</span>(<span class="fu">op_zeros</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a><span class="co"># This is the loss created during the call above</span></span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;losses:&quot;</span>, <span class="fu">length</span>(layer<span class="sc">$</span>losses), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## losses: 1</code></pre>
<p>In addition, the <code>loss</code> property also contains
regularization losses created for the weights of any inner layer:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a>layer_outer_with_kernel_regularizer <span class="ot">&lt;-</span> <span class="fu">Layer</span>(</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a>  <span class="st">&quot;OuterLayerWithKernelRegularizer&quot;</span>,</span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>() {</span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>()</span>
<span id="cb34-5"><a href="#cb34-5" tabindex="-1"></a>    self<span class="sc">$</span>dense <span class="ot">&lt;-</span> <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">32</span>,</span>
<span id="cb34-6"><a href="#cb34-6" tabindex="-1"></a>                              <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l2</span>(<span class="fl">1e-3</span>))</span>
<span id="cb34-7"><a href="#cb34-7" tabindex="-1"></a>  },</span>
<span id="cb34-8"><a href="#cb34-8" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb34-9"><a href="#cb34-9" tabindex="-1"></a>    self<span class="sc">$</span><span class="fu">dense</span>(inputs)</span>
<span id="cb34-10"><a href="#cb34-10" tabindex="-1"></a>  }</span>
<span id="cb34-11"><a href="#cb34-11" tabindex="-1"></a>)</span>
<span id="cb34-12"><a href="#cb34-12" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" tabindex="-1"></a>layer <span class="ot">&lt;-</span> <span class="fu">layer_outer_with_kernel_regularizer</span>()</span>
<span id="cb34-14"><a href="#cb34-14" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">layer</span>(<span class="fu">op_zeros</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb34-15"><a href="#cb34-15" tabindex="-1"></a></span>
<span id="cb34-16"><a href="#cb34-16" tabindex="-1"></a><span class="co"># This is `1e-3 * sum(layer$dense$kernel ** 2)`,</span></span>
<span id="cb34-17"><a href="#cb34-17" tabindex="-1"></a><span class="co"># created by the `kernel_regularizer` above.</span></span>
<span id="cb34-18"><a href="#cb34-18" tabindex="-1"></a><span class="fu">print</span>(layer<span class="sc">$</span>losses)</span></code></pre></div>
<pre><code>## [[1]]
## tf.Tensor(0.002025157, shape=(), dtype=float32)</code></pre>
<p>These losses are meant to be taken into account when writing custom
training loops.</p>
<p>They also work seamlessly with <code>fit()</code> (they get
automatically summed and added to the main loss, if any):</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a>inputs <span class="ot">&lt;-</span> <span class="fu">keras_input</span>(<span class="at">shape =</span> <span class="dv">3</span>)</span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a>outputs <span class="ot">&lt;-</span> inputs <span class="sc">|&gt;</span> <span class="fu">layer_activity_regularization</span>()</span>
<span id="cb36-3"><a href="#cb36-3" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model</span>(inputs, outputs)</span>
<span id="cb36-4"><a href="#cb36-4" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" tabindex="-1"></a><span class="co"># If there is a loss passed in `compile`, the regularization</span></span>
<span id="cb36-6"><a href="#cb36-6" tabindex="-1"></a><span class="co"># losses get added to it</span></span>
<span id="cb36-7"><a href="#cb36-7" tabindex="-1"></a>model <span class="sc">|&gt;</span> <span class="fu">compile</span>(<span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>, <span class="at">loss =</span> <span class="st">&quot;mse&quot;</span>)</span>
<span id="cb36-8"><a href="#cb36-8" tabindex="-1"></a>model <span class="sc">|&gt;</span> <span class="fu">fit</span>(<span class="fu">random_normal</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>)), <span class="fu">random_normal</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>)), <span class="at">epochs =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## 1/1 - 0s - 142ms/step - loss: 1.9081</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a><span class="co"># It&#39;s also possible not to pass any loss in `compile`,</span></span>
<span id="cb38-2"><a href="#cb38-2" tabindex="-1"></a><span class="co"># since the model already has a loss to minimize, via the `add_loss`</span></span>
<span id="cb38-3"><a href="#cb38-3" tabindex="-1"></a><span class="co"># call during the forward pass!</span></span>
<span id="cb38-4"><a href="#cb38-4" tabindex="-1"></a>model <span class="sc">|&gt;</span> <span class="fu">compile</span>(<span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>)</span>
<span id="cb38-5"><a href="#cb38-5" tabindex="-1"></a>model <span class="sc">|&gt;</span> <span class="fu">fit</span>(<span class="fu">random_normal</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>)), <span class="fu">random_normal</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>)), <span class="at">epochs =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## 1/1 - 0s - 115ms/step - loss: 1.6613</code></pre>
</div>
<div id="you-can-optionally-enable-serialization-on-your-layers" class="section level2">
<h2>You can optionally enable serialization on your layers</h2>
<p>If you need your custom layers to be serializable as part of a <a href="functional_api.html">Functional model</a>, you can optionally
implement a <code>get_config()</code> method:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a>layer_linear <span class="ot">&lt;-</span> <span class="fu">Layer</span>(</span>
<span id="cb40-2"><a href="#cb40-2" tabindex="-1"></a>  <span class="st">&quot;Linear&quot;</span>,</span>
<span id="cb40-3"><a href="#cb40-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(<span class="at">units =</span> <span class="dv">32</span>) {</span>
<span id="cb40-4"><a href="#cb40-4" tabindex="-1"></a>    self<span class="sc">$</span>units <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(units)</span>
<span id="cb40-5"><a href="#cb40-5" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>()</span>
<span id="cb40-6"><a href="#cb40-6" tabindex="-1"></a>  },</span>
<span id="cb40-7"><a href="#cb40-7" tabindex="-1"></a>  <span class="at">build =</span> <span class="cf">function</span>(input_shape) {</span>
<span id="cb40-8"><a href="#cb40-8" tabindex="-1"></a>    self<span class="sc">$</span>w <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(</span>
<span id="cb40-9"><a href="#cb40-9" tabindex="-1"></a>      <span class="at">shape =</span> <span class="fu">shape</span>(<span class="fu">tail</span>(input_shape, <span class="dv">1</span>), self<span class="sc">$</span>units),</span>
<span id="cb40-10"><a href="#cb40-10" tabindex="-1"></a>      <span class="at">initializer =</span> <span class="st">&quot;random_normal&quot;</span>,</span>
<span id="cb40-11"><a href="#cb40-11" tabindex="-1"></a>      <span class="at">trainable =</span> <span class="cn">TRUE</span></span>
<span id="cb40-12"><a href="#cb40-12" tabindex="-1"></a>    )</span>
<span id="cb40-13"><a href="#cb40-13" tabindex="-1"></a>    self<span class="sc">$</span>b <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(</span>
<span id="cb40-14"><a href="#cb40-14" tabindex="-1"></a>      <span class="at">shape =</span> <span class="fu">shape</span>(self<span class="sc">$</span>units),</span>
<span id="cb40-15"><a href="#cb40-15" tabindex="-1"></a>      <span class="at">initializer =</span> <span class="st">&quot;zeros&quot;</span>,</span>
<span id="cb40-16"><a href="#cb40-16" tabindex="-1"></a>      <span class="at">trainable =</span> <span class="cn">TRUE</span></span>
<span id="cb40-17"><a href="#cb40-17" tabindex="-1"></a>    )</span>
<span id="cb40-18"><a href="#cb40-18" tabindex="-1"></a>  },</span>
<span id="cb40-19"><a href="#cb40-19" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb40-20"><a href="#cb40-20" tabindex="-1"></a>    <span class="fu">op_matmul</span>(inputs, self<span class="sc">$</span>w) <span class="sc">+</span> self<span class="sc">$</span>b</span>
<span id="cb40-21"><a href="#cb40-21" tabindex="-1"></a>  },</span>
<span id="cb40-22"><a href="#cb40-22" tabindex="-1"></a>  <span class="at">get_config =</span> <span class="cf">function</span>() {</span>
<span id="cb40-23"><a href="#cb40-23" tabindex="-1"></a>    <span class="fu">list</span>(<span class="at">units =</span> self<span class="sc">$</span>units)</span>
<span id="cb40-24"><a href="#cb40-24" tabindex="-1"></a>  }</span>
<span id="cb40-25"><a href="#cb40-25" tabindex="-1"></a>)</span>
<span id="cb40-26"><a href="#cb40-26" tabindex="-1"></a></span>
<span id="cb40-27"><a href="#cb40-27" tabindex="-1"></a><span class="co"># Now you can recreate the layer from its config:</span></span>
<span id="cb40-28"><a href="#cb40-28" tabindex="-1"></a>layer <span class="ot">&lt;-</span> <span class="fu">layer_linear</span>(<span class="at">units =</span> <span class="dv">64</span>)</span>
<span id="cb40-29"><a href="#cb40-29" tabindex="-1"></a>config <span class="ot">&lt;-</span> <span class="fu">get_config</span>(layer)</span>
<span id="cb40-30"><a href="#cb40-30" tabindex="-1"></a><span class="fu">str</span>(config)</span></code></pre></div>
<pre><code>## List of 1
##  $ units: int 64
##  - attr(*, &quot;__class__&quot;)=&lt;class &#39;&lt;r-globalenv&gt;.Linear&#39;&gt;</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a>new_layer <span class="ot">&lt;-</span> <span class="fu">from_config</span>(config)</span></code></pre></div>
<p>Note that the <code>initialize()</code> method of the base
<code>Layer</code> class takes some keyword arguments, in particular a
<code>name</code> and a <code>dtype</code>. It’s good practice to pass
these arguments to the parent class in <code>initialize()</code> and to
include them in the layer config:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" tabindex="-1"></a>Linear <span class="ot">&lt;-</span> <span class="fu">new_layer_class</span>(</span>
<span id="cb43-2"><a href="#cb43-2" tabindex="-1"></a>  <span class="st">&quot;Linear&quot;</span>,</span>
<span id="cb43-3"><a href="#cb43-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(<span class="at">units =</span> <span class="dv">32</span>, ...) {</span>
<span id="cb43-4"><a href="#cb43-4" tabindex="-1"></a>    self<span class="sc">$</span>units <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(units)</span>
<span id="cb43-5"><a href="#cb43-5" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb43-6"><a href="#cb43-6" tabindex="-1"></a>  },</span>
<span id="cb43-7"><a href="#cb43-7" tabindex="-1"></a>  <span class="at">build =</span> <span class="cf">function</span>(input_shape) {</span>
<span id="cb43-8"><a href="#cb43-8" tabindex="-1"></a>    self<span class="sc">$</span>w <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(</span>
<span id="cb43-9"><a href="#cb43-9" tabindex="-1"></a>      <span class="at">shape =</span> <span class="fu">shape</span>(<span class="fu">tail</span>(input_shape, <span class="dv">1</span>), self<span class="sc">$</span>units),</span>
<span id="cb43-10"><a href="#cb43-10" tabindex="-1"></a>      <span class="at">initializer =</span> <span class="st">&quot;random_normal&quot;</span>,</span>
<span id="cb43-11"><a href="#cb43-11" tabindex="-1"></a>      <span class="at">trainable =</span> <span class="cn">TRUE</span></span>
<span id="cb43-12"><a href="#cb43-12" tabindex="-1"></a>    )</span>
<span id="cb43-13"><a href="#cb43-13" tabindex="-1"></a>    self<span class="sc">$</span>b <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(</span>
<span id="cb43-14"><a href="#cb43-14" tabindex="-1"></a>      <span class="at">shape =</span> <span class="fu">shape</span>(self<span class="sc">$</span>units),</span>
<span id="cb43-15"><a href="#cb43-15" tabindex="-1"></a>      <span class="at">initializer =</span> <span class="st">&quot;zeros&quot;</span>,</span>
<span id="cb43-16"><a href="#cb43-16" tabindex="-1"></a>      <span class="at">trainable =</span> <span class="cn">TRUE</span></span>
<span id="cb43-17"><a href="#cb43-17" tabindex="-1"></a>    )</span>
<span id="cb43-18"><a href="#cb43-18" tabindex="-1"></a>  },</span>
<span id="cb43-19"><a href="#cb43-19" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb43-20"><a href="#cb43-20" tabindex="-1"></a>    <span class="fu">op_matmul</span>(inputs, self<span class="sc">$</span>w) <span class="sc">+</span> self<span class="sc">$</span>b</span>
<span id="cb43-21"><a href="#cb43-21" tabindex="-1"></a>  },</span>
<span id="cb43-22"><a href="#cb43-22" tabindex="-1"></a>  <span class="at">get_config =</span> <span class="cf">function</span>() {</span>
<span id="cb43-23"><a href="#cb43-23" tabindex="-1"></a>    <span class="fu">list</span>(<span class="at">units =</span> self<span class="sc">$</span>units)</span>
<span id="cb43-24"><a href="#cb43-24" tabindex="-1"></a>  }</span>
<span id="cb43-25"><a href="#cb43-25" tabindex="-1"></a>)</span>
<span id="cb43-26"><a href="#cb43-26" tabindex="-1"></a></span>
<span id="cb43-27"><a href="#cb43-27" tabindex="-1"></a>layer <span class="ot">&lt;-</span> <span class="fu">Linear</span>(<span class="at">units =</span> <span class="dv">64</span>)</span>
<span id="cb43-28"><a href="#cb43-28" tabindex="-1"></a>config <span class="ot">&lt;-</span> <span class="fu">get_config</span>(layer)</span>
<span id="cb43-29"><a href="#cb43-29" tabindex="-1"></a><span class="fu">str</span>(config)</span></code></pre></div>
<pre><code>## List of 1
##  $ units: int 64
##  - attr(*, &quot;__class__&quot;)=&lt;class &#39;&lt;r-globalenv&gt;.Linear&#39;&gt;</code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" tabindex="-1"></a>new_layer <span class="ot">&lt;-</span> <span class="fu">from_config</span>(config)</span></code></pre></div>
<p>If you need more flexibility when deserializing the layer from its
config, you can also override the <code>from_config()</code> class
method. This is the base implementation of
<code>from_config()</code>:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" tabindex="-1"></a><span class="fu">Layer</span>(</span>
<span id="cb46-2"><a href="#cb46-2" tabindex="-1"></a>  ...,</span>
<span id="cb46-3"><a href="#cb46-3" tabindex="-1"></a>  <span class="at">from_config =</span> <span class="cf">function</span>(config) {</span>
<span id="cb46-4"><a href="#cb46-4" tabindex="-1"></a>    <span class="co"># calling `__class__`() creates a new instance and calls initialize()</span></span>
<span id="cb46-5"><a href="#cb46-5" tabindex="-1"></a>    <span class="fu">do.call</span>(<span class="st">`</span><span class="at">__class__</span><span class="st">`</span>, config)</span>
<span id="cb46-6"><a href="#cb46-6" tabindex="-1"></a>  }</span>
<span id="cb46-7"><a href="#cb46-7" tabindex="-1"></a>)</span></code></pre></div>
<p>To learn more about serialization and saving, see the complete <a href="serialization_and_saving.html">guide to saving and serializing
models</a>.</p>
</div>
<div id="privileged-training-argument-in-the-call-method" class="section level2">
<h2>Privileged <code>training</code> argument in the <code>call()</code>
method</h2>
<p>Some layers, in particular the <code>BatchNormalization</code> layer
and the <code>Dropout</code> layer, have different behaviors during
training and inference. For such layers, it is standard practice to
expose a <code>training</code> (boolean) argument in the
<code>call()</code> method.</p>
<p>By exposing this argument in <code>call()</code>, you enable the
built-in training and evaluation loops (e.g. <code>fit()</code>) to
correctly use the layer in training and inference.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" tabindex="-1"></a>layer_custom_dropout <span class="ot">&lt;-</span> <span class="fu">Layer</span>(</span>
<span id="cb47-2"><a href="#cb47-2" tabindex="-1"></a>  <span class="st">&quot;CustomDropout&quot;</span>,</span>
<span id="cb47-3"><a href="#cb47-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(rate, ...) {</span>
<span id="cb47-4"><a href="#cb47-4" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb47-5"><a href="#cb47-5" tabindex="-1"></a>    self<span class="sc">$</span>rate <span class="ot">&lt;-</span> rate</span>
<span id="cb47-6"><a href="#cb47-6" tabindex="-1"></a>    self<span class="sc">$</span>seed_generator <span class="ot">&lt;-</span> <span class="fu">random_seed_generator</span>(<span class="dv">1337</span>)</span>
<span id="cb47-7"><a href="#cb47-7" tabindex="-1"></a>  },</span>
<span id="cb47-8"><a href="#cb47-8" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs, <span class="at">training =</span> <span class="cn">NULL</span>) {</span>
<span id="cb47-9"><a href="#cb47-9" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">isTRUE</span>(training))</span>
<span id="cb47-10"><a href="#cb47-10" tabindex="-1"></a>      <span class="fu">return</span>(<span class="fu">random_dropout</span>(inputs, <span class="at">rate =</span> self<span class="sc">$</span>rate,</span>
<span id="cb47-11"><a href="#cb47-11" tabindex="-1"></a>                            <span class="at">seed =</span> self.seed_generator))</span>
<span id="cb47-12"><a href="#cb47-12" tabindex="-1"></a>    inputs</span>
<span id="cb47-13"><a href="#cb47-13" tabindex="-1"></a>  }</span>
<span id="cb47-14"><a href="#cb47-14" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div id="privileged-mask-argument-in-the-call-method" class="section level2">
<h2>Privileged <code>mask</code> argument in the <code>call()</code>
method</h2>
<p>The other privileged argument supported by <code>call()</code> is the
<code>mask</code> argument.</p>
<p>You will find it in all Keras RNN layers. A mask is a boolean tensor
(one boolean value per timestep in the input) used to skip certain input
timesteps when processing timeseries data.</p>
<p>Keras will automatically pass the correct <code>mask</code> argument
to <code>call()</code> for layers that support it, when a mask is
generated by a prior layer. Mask-generating layers are the
<code>Embedding</code> layer configured with
<code>mask_zero = TRUE</code>, and the <code>Masking</code> layer.</p>
</div>
<div id="the-model-class" class="section level2">
<h2>The <code>Model</code> class</h2>
<p>In general, you will use the <code>Layer</code> class to define inner
computation blocks, and will use the <code>Model</code> class to define
the outer model – the object you will train.</p>
<p>For instance, in a ResNet50 model, you would have several ResNet
blocks subclassing <code>Layer</code>, and a single <code>Model</code>
encompassing the entire ResNet50 network.</p>
<p>The <code>Model</code> class has the same API as <code>Layer</code>,
with the following differences:</p>
<ul>
<li>It exposes built-in training, evaluation, and prediction loops
(<code>fit()</code>, <code>evaluate()</code>,
<code>predict()</code>).</li>
<li>It exposes the list of its inner layers, via the
<code>model$layers</code> property.</li>
<li>It exposes saving and serialization APIs (<code>save()</code>,
<code>save_weights()</code>…)</li>
</ul>
<p>Effectively, the <code>Layer</code> class corresponds to what we
refer to in the literature as a “layer” (as in “convolution layer” or
“recurrent layer”) or as a “block” (as in “ResNet block” or “Inception
block”).</p>
<p>Meanwhile, the <code>Model</code> class corresponds to what is
referred to in the literature as a “model” (as in “deep learning model”)
or as a “network” (as in “deep neural network”).</p>
<p>So if you’re wondering, “should I use the <code>Layer</code> class or
the <code>Model</code> class?”, ask yourself: will I need to call
<code>fit()</code> on it? Will I need to call <code>save()</code> on it?
If so, go with <code>Model</code>. If not (either because your class is
just a block in a bigger system, or because you are writing training
&amp; saving code yourself), use <code>Layer</code>.</p>
<p>For instance, we could take our mini-resnet example above, and use it
to build a <code>Model</code> that we could train with
<code>fit()</code>, and that we could save with
<code>save_weights()</code>:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" tabindex="-1"></a>ResNet <span class="ot">&lt;-</span> <span class="fu">Model</span>(</span>
<span id="cb48-2"><a href="#cb48-2" tabindex="-1"></a>  <span class="st">&quot;ResNet&quot;</span>,</span>
<span id="cb48-3"><a href="#cb48-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(<span class="at">num_classes =</span> <span class="dv">1000</span>, ...) {</span>
<span id="cb48-4"><a href="#cb48-4" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb48-5"><a href="#cb48-5" tabindex="-1"></a>    self<span class="sc">$</span>block_1 <span class="ot">&lt;-</span> <span class="fu">layer_resnet_block</span>()</span>
<span id="cb48-6"><a href="#cb48-6" tabindex="-1"></a>    self<span class="sc">$</span>block_2 <span class="ot">&lt;-</span> <span class="fu">layer_resnet_block</span>()</span>
<span id="cb48-7"><a href="#cb48-7" tabindex="-1"></a>    self<span class="sc">$</span>global_pool <span class="ot">&lt;-</span> <span class="fu">layer_global_average_pooling_2d</span>()</span>
<span id="cb48-8"><a href="#cb48-8" tabindex="-1"></a>    self<span class="sc">$</span>classifier <span class="ot">&lt;-</span> <span class="fu">layer_dense</span>(num_classes)</span>
<span id="cb48-9"><a href="#cb48-9" tabindex="-1"></a>  },</span>
<span id="cb48-10"><a href="#cb48-10" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb48-11"><a href="#cb48-11" tabindex="-1"></a>    inputs <span class="sc">|&gt;</span></span>
<span id="cb48-12"><a href="#cb48-12" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">block_1</span>() <span class="sc">|&gt;</span></span>
<span id="cb48-13"><a href="#cb48-13" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">block_2</span>() <span class="sc">|&gt;</span></span>
<span id="cb48-14"><a href="#cb48-14" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">global_pool</span>() <span class="sc">|&gt;</span></span>
<span id="cb48-15"><a href="#cb48-15" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">classifier</span>()</span>
<span id="cb48-16"><a href="#cb48-16" tabindex="-1"></a>  }</span>
<span id="cb48-17"><a href="#cb48-17" tabindex="-1"></a>)</span>
<span id="cb48-18"><a href="#cb48-18" tabindex="-1"></a></span>
<span id="cb48-19"><a href="#cb48-19" tabindex="-1"></a>resnet <span class="ot">&lt;-</span> <span class="fu">ResNet</span>()</span>
<span id="cb48-20"><a href="#cb48-20" tabindex="-1"></a>dataset <span class="ot">&lt;-</span> ...</span>
<span id="cb48-21"><a href="#cb48-21" tabindex="-1"></a>resnet <span class="sc">|&gt;</span> <span class="fu">fit</span>(dataset, <span class="at">epochs=</span><span class="dv">10</span>)</span>
<span id="cb48-22"><a href="#cb48-22" tabindex="-1"></a>resnet <span class="sc">|&gt;</span> <span class="fu">save_model</span>(<span class="st">&quot;filepath.keras&quot;</span>)</span></code></pre></div>
</div>
<div id="putting-it-all-together-an-end-to-end-example" class="section level2">
<h2>Putting it all together: an end-to-end example</h2>
<p>Here’s what you’ve learned so far:</p>
<ul>
<li>A <code>Layer</code> encapsulate a state (created in
<code>initialize()</code> or <code>build()</code>) and some computation
(defined in <code>call()</code>).</li>
<li>Layers can be recursively nested to create new, bigger computation
blocks.</li>
<li>Layers are backend-agnostic as long as they only use Keras APIs. You
can use backend-native APIs (such as <code>jax$numpy</code>,
<code>torch$nn</code> or <code>tf$nn</code>), but then your layer will
only be usable with that specific backend.</li>
<li>Layers can create and track losses (typically regularization losses)
via <code>add_loss()</code>.</li>
<li>The outer container, the thing you want to train, is a
<code>Model</code>. A <code>Model</code> is just like a
<code>Layer</code>, but with added training and serialization
utilities.</li>
</ul>
<p>Let’s put all of these things together into an end-to-end example:
we’re going to implement a Variational AutoEncoder (VAE) in a
backend-agnostic fashion – so that it runs the same with TensorFlow,
JAX, and PyTorch. We’ll train it on MNIST digits.</p>
<p>Our VAE will be a subclass of <code>Model</code>, built as a nested
composition of layers that subclass <code>Layer</code>. It will feature
a regularization loss (KL divergence).</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" tabindex="-1"></a>layer_sampling <span class="ot">&lt;-</span> <span class="fu">Layer</span>(</span>
<span id="cb49-2"><a href="#cb49-2" tabindex="-1"></a>  <span class="st">&quot;Sampling&quot;</span>,</span>
<span id="cb49-3"><a href="#cb49-3" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(...) {</span>
<span id="cb49-4"><a href="#cb49-4" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb49-5"><a href="#cb49-5" tabindex="-1"></a>    self<span class="sc">$</span>seed_generator <span class="ot">&lt;-</span> <span class="fu">random_seed_generator</span>(<span class="dv">1337</span>)</span>
<span id="cb49-6"><a href="#cb49-6" tabindex="-1"></a>  },</span>
<span id="cb49-7"><a href="#cb49-7" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb49-8"><a href="#cb49-8" tabindex="-1"></a>    <span class="fu">c</span>(z_mean, z_log_var) <span class="sc">%&lt;-%</span> inputs</span>
<span id="cb49-9"><a href="#cb49-9" tabindex="-1"></a>    batch <span class="ot">&lt;-</span> <span class="fu">op_shape</span>(z_mean)[[<span class="dv">1</span>]]</span>
<span id="cb49-10"><a href="#cb49-10" tabindex="-1"></a>    dim <span class="ot">&lt;-</span> <span class="fu">op_shape</span>(z_mean)[[<span class="dv">2</span>]]</span>
<span id="cb49-11"><a href="#cb49-11" tabindex="-1"></a>    epsilon <span class="ot">&lt;-</span> <span class="fu">random_normal</span>(<span class="at">shape =</span> <span class="fu">c</span>(batch, dim),</span>
<span id="cb49-12"><a href="#cb49-12" tabindex="-1"></a>                             <span class="at">seed=</span>self<span class="sc">$</span>seed_generator)</span>
<span id="cb49-13"><a href="#cb49-13" tabindex="-1"></a>    z_mean <span class="sc">+</span> <span class="fu">op_exp</span>(<span class="fl">0.5</span> <span class="sc">*</span> z_log_var) <span class="sc">*</span> epsilon</span>
<span id="cb49-14"><a href="#cb49-14" tabindex="-1"></a>  }</span>
<span id="cb49-15"><a href="#cb49-15" tabindex="-1"></a>)</span>
<span id="cb49-16"><a href="#cb49-16" tabindex="-1"></a></span>
<span id="cb49-17"><a href="#cb49-17" tabindex="-1"></a><span class="co"># Maps MNIST digits to a triplet (z_mean, z_log_var, z).</span></span>
<span id="cb49-18"><a href="#cb49-18" tabindex="-1"></a>layer_encoder <span class="ot">&lt;-</span> <span class="fu">Layer</span>(</span>
<span id="cb49-19"><a href="#cb49-19" tabindex="-1"></a>  <span class="st">&quot;Encoder&quot;</span>,</span>
<span id="cb49-20"><a href="#cb49-20" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(<span class="at">latent_dim =</span> <span class="dv">32</span>, <span class="at">intermediate_dim =</span> <span class="dv">64</span>, ...) {</span>
<span id="cb49-21"><a href="#cb49-21" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb49-22"><a href="#cb49-22" tabindex="-1"></a>    self<span class="sc">$</span>dense_proj <span class="ot">&lt;-</span></span>
<span id="cb49-23"><a href="#cb49-23" tabindex="-1"></a>      <span class="fu">layer_dense</span>(<span class="at">units =</span> intermediate_dim,  <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>)</span>
<span id="cb49-24"><a href="#cb49-24" tabindex="-1"></a>    self<span class="sc">$</span>dense_mean <span class="ot">&lt;-</span> <span class="fu">layer_dense</span>(<span class="at">units =</span> latent_dim)</span>
<span id="cb49-25"><a href="#cb49-25" tabindex="-1"></a>    self<span class="sc">$</span>dense_log_var <span class="ot">&lt;-</span> <span class="fu">layer_dense</span>(<span class="at">units =</span> latent_dim)</span>
<span id="cb49-26"><a href="#cb49-26" tabindex="-1"></a>    self<span class="sc">$</span>sampling <span class="ot">&lt;-</span> <span class="fu">layer_sampling</span>()</span>
<span id="cb49-27"><a href="#cb49-27" tabindex="-1"></a>  },</span>
<span id="cb49-28"><a href="#cb49-28" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb49-29"><a href="#cb49-29" tabindex="-1"></a>    x <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">dense_proj</span>(inputs)</span>
<span id="cb49-30"><a href="#cb49-30" tabindex="-1"></a>    z_mean <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">dense_mean</span>(x)</span>
<span id="cb49-31"><a href="#cb49-31" tabindex="-1"></a>    z_log_var <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">dense_log_var</span>(x)</span>
<span id="cb49-32"><a href="#cb49-32" tabindex="-1"></a>    z <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">sampling</span>(<span class="fu">list</span>(z_mean, z_log_var))</span>
<span id="cb49-33"><a href="#cb49-33" tabindex="-1"></a>    <span class="fu">list</span>(z_mean, z_log_var, z)</span>
<span id="cb49-34"><a href="#cb49-34" tabindex="-1"></a>  }</span>
<span id="cb49-35"><a href="#cb49-35" tabindex="-1"></a>)</span>
<span id="cb49-36"><a href="#cb49-36" tabindex="-1"></a></span>
<span id="cb49-37"><a href="#cb49-37" tabindex="-1"></a><span class="co"># Converts z, the encoded digit vector, back into a readable digit.</span></span>
<span id="cb49-38"><a href="#cb49-38" tabindex="-1"></a>layer_decoder <span class="ot">&lt;-</span> <span class="fu">Layer</span>(</span>
<span id="cb49-39"><a href="#cb49-39" tabindex="-1"></a>  <span class="st">&quot;Decoder&quot;</span>,</span>
<span id="cb49-40"><a href="#cb49-40" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(original_dim, <span class="at">intermediate_dim =</span> <span class="dv">64</span>, ...) {</span>
<span id="cb49-41"><a href="#cb49-41" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb49-42"><a href="#cb49-42" tabindex="-1"></a>    self<span class="sc">$</span>dense_proj <span class="ot">&lt;-</span></span>
<span id="cb49-43"><a href="#cb49-43" tabindex="-1"></a>      <span class="fu">layer_dense</span>(<span class="at">units =</span> intermediate_dim, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>)</span>
<span id="cb49-44"><a href="#cb49-44" tabindex="-1"></a>    self<span class="sc">$</span>dense_output <span class="ot">&lt;-</span></span>
<span id="cb49-45"><a href="#cb49-45" tabindex="-1"></a>      <span class="fu">layer_dense</span>(<span class="at">units =</span> original_dim, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb49-46"><a href="#cb49-46" tabindex="-1"></a>  },</span>
<span id="cb49-47"><a href="#cb49-47" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb49-48"><a href="#cb49-48" tabindex="-1"></a>    x <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">dense_proj</span>(inputs)</span>
<span id="cb49-49"><a href="#cb49-49" tabindex="-1"></a>    self<span class="sc">$</span><span class="fu">dense_output</span>(x)</span>
<span id="cb49-50"><a href="#cb49-50" tabindex="-1"></a>  }</span>
<span id="cb49-51"><a href="#cb49-51" tabindex="-1"></a>)</span>
<span id="cb49-52"><a href="#cb49-52" tabindex="-1"></a></span>
<span id="cb49-53"><a href="#cb49-53" tabindex="-1"></a><span class="co"># Combines the encoder and decoder into an end-to-end model for training.</span></span>
<span id="cb49-54"><a href="#cb49-54" tabindex="-1"></a>VariationalAutoEncoder <span class="ot">&lt;-</span> <span class="fu">Model</span>(</span>
<span id="cb49-55"><a href="#cb49-55" tabindex="-1"></a>  <span class="st">&quot;VariationalAutoEncoder&quot;</span>,</span>
<span id="cb49-56"><a href="#cb49-56" tabindex="-1"></a></span>
<span id="cb49-57"><a href="#cb49-57" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(original_dim, <span class="at">intermediate_dim =</span> <span class="dv">64</span>, <span class="at">latent_dim =</span> <span class="dv">32</span>,</span>
<span id="cb49-58"><a href="#cb49-58" tabindex="-1"></a>                        <span class="at">name =</span> <span class="st">&quot;autoencoder&quot;</span>, ...) {</span>
<span id="cb49-59"><a href="#cb49-59" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(<span class="at">name =</span> name, ...)</span>
<span id="cb49-60"><a href="#cb49-60" tabindex="-1"></a>    self<span class="sc">$</span>original_dim <span class="ot">&lt;-</span> original_dim</span>
<span id="cb49-61"><a href="#cb49-61" tabindex="-1"></a>    self<span class="sc">$</span>encoder <span class="ot">&lt;-</span> <span class="fu">layer_encoder</span>(<span class="at">latent_dim =</span> latent_dim,</span>
<span id="cb49-62"><a href="#cb49-62" tabindex="-1"></a>                            <span class="at">intermediate_dim =</span> intermediate_dim)</span>
<span id="cb49-63"><a href="#cb49-63" tabindex="-1"></a>    self<span class="sc">$</span>decoder <span class="ot">&lt;-</span> <span class="fu">layer_decoder</span>(<span class="at">original_dim =</span> original_dim,</span>
<span id="cb49-64"><a href="#cb49-64" tabindex="-1"></a>                            <span class="at">intermediate_dim =</span> intermediate_dim)</span>
<span id="cb49-65"><a href="#cb49-65" tabindex="-1"></a>  },</span>
<span id="cb49-66"><a href="#cb49-66" tabindex="-1"></a></span>
<span id="cb49-67"><a href="#cb49-67" tabindex="-1"></a>  <span class="at">call =</span> <span class="cf">function</span>(inputs) {</span>
<span id="cb49-68"><a href="#cb49-68" tabindex="-1"></a>    <span class="fu">c</span>(z_mean, z_log_var, z) <span class="sc">%&lt;-%</span> self<span class="sc">$</span><span class="fu">encoder</span>(inputs)</span>
<span id="cb49-69"><a href="#cb49-69" tabindex="-1"></a>    reconstructed <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">decoder</span>(z)</span>
<span id="cb49-70"><a href="#cb49-70" tabindex="-1"></a>    <span class="co"># Add KL divergence regularization loss.</span></span>
<span id="cb49-71"><a href="#cb49-71" tabindex="-1"></a>    kl_loss <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">op_mean</span>(z_log_var <span class="sc">-</span> <span class="fu">op_square</span>(z_mean) <span class="sc">-</span> <span class="fu">op_exp</span>(z_log_var) <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb49-72"><a href="#cb49-72" tabindex="-1"></a>    self<span class="sc">$</span><span class="fu">add_loss</span>(kl_loss)</span>
<span id="cb49-73"><a href="#cb49-73" tabindex="-1"></a>    reconstructed</span>
<span id="cb49-74"><a href="#cb49-74" tabindex="-1"></a>  }</span>
<span id="cb49-75"><a href="#cb49-75" tabindex="-1"></a>)</span></code></pre></div>
<p>Let’s train it on MNIST using the <code>fit()</code> API:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">c</span>(x_train, .), .) <span class="sc">%&lt;-%</span> <span class="fu">dataset_mnist</span>()</span>
<span id="cb50-2"><a href="#cb50-2" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> x_train <span class="sc">|&gt;</span></span>
<span id="cb50-3"><a href="#cb50-3" tabindex="-1"></a>  <span class="fu">op_reshape</span>(<span class="fu">c</span>(<span class="dv">60000</span>, <span class="dv">784</span>)) <span class="sc">|&gt;</span></span>
<span id="cb50-4"><a href="#cb50-4" tabindex="-1"></a>  <span class="fu">op_cast</span>(<span class="st">&quot;float32&quot;</span>) <span class="sc">|&gt;</span></span>
<span id="cb50-5"><a href="#cb50-5" tabindex="-1"></a>  <span class="fu">op_divide</span>(<span class="dv">255</span>)</span>
<span id="cb50-6"><a href="#cb50-6" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" tabindex="-1"></a>original_dim <span class="ot">&lt;-</span> <span class="dv">784</span></span>
<span id="cb50-8"><a href="#cb50-8" tabindex="-1"></a>vae <span class="ot">&lt;-</span> <span class="fu">VariationalAutoEncoder</span>(</span>
<span id="cb50-9"><a href="#cb50-9" tabindex="-1"></a>  <span class="at">original_dim =</span> <span class="dv">784</span>,</span>
<span id="cb50-10"><a href="#cb50-10" tabindex="-1"></a>  <span class="at">intermediate_dim =</span> <span class="dv">64</span>,</span>
<span id="cb50-11"><a href="#cb50-11" tabindex="-1"></a>  <span class="at">latent_dim =</span> <span class="dv">32</span></span>
<span id="cb50-12"><a href="#cb50-12" tabindex="-1"></a>)</span>
<span id="cb50-13"><a href="#cb50-13" tabindex="-1"></a></span>
<span id="cb50-14"><a href="#cb50-14" tabindex="-1"></a>optimizer <span class="ot">&lt;-</span> <span class="fu">optimizer_adam</span>(<span class="at">learning_rate =</span> <span class="fl">1e-3</span>)</span>
<span id="cb50-15"><a href="#cb50-15" tabindex="-1"></a>vae <span class="sc">|&gt;</span> <span class="fu">compile</span>(optimizer, <span class="at">loss =</span> <span class="fu">loss_mean_squared_error</span>())</span>
<span id="cb50-16"><a href="#cb50-16" tabindex="-1"></a></span>
<span id="cb50-17"><a href="#cb50-17" tabindex="-1"></a>vae <span class="sc">|&gt;</span> <span class="fu">fit</span>(x_train, x_train, <span class="at">epochs =</span> <span class="dv">2</span>, <span class="at">batch_size =</span> <span class="dv">64</span>)</span></code></pre></div>
<pre><code>## Epoch 1/2
## 938/938 - 5s - 5ms/step - loss: 0.0748
## Epoch 2/2
## 938/938 - 1s - 2ms/step - loss: 0.0676</code></pre>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
