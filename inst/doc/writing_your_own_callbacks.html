<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Writing your own callbacks</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Writing your own callbacks</h1>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>A callback is a powerful tool to customize the behavior of a Keras
model during training, evaluation, or inference. Examples include
<code>keras.callbacks.TensorBoard</code> to visualize training progress
and results with TensorBoard, or
<code>keras.callbacks.ModelCheckpoint</code> to periodically save your
model during training.</p>
<p>In this guide, you will learn what a Keras callback is, what it can
do, and how you can build your own. We provide a few demos of simple
callback applications to get you started.</p>
</div>
<div id="setup" class="section level2">
<h2>Setup</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(keras3)</span></code></pre></div>
</div>
<div id="keras-callbacks-overview" class="section level2">
<h2>Keras callbacks overview</h2>
<p>All callbacks subclass the <code>keras.callbacks.Callback</code>
class, and override a set of methods called at various stages of
training, testing, and predicting. Callbacks are useful to get a view on
internal states and statistics of the model during training.</p>
<p>You can pass a list of callbacks (as the keyword argument
<code>callbacks</code>) to the following model methods:</p>
<ul>
<li><code>fit()</code></li>
<li><code>evaluate()</code></li>
<li><code>predict()</code></li>
</ul>
</div>
<div id="an-overview-of-callback-methods" class="section level2">
<h2>An overview of callback methods</h2>
<div id="global-methods" class="section level3">
<h3>Global methods</h3>
<div id="on_traintestpredict_beginlogs-null" class="section level4">
<h4><code>on_(train|test|predict)_begin(logs = NULL)</code></h4>
<p>Called at the beginning of
<code>fit</code>/<code>evaluate</code>/<code>predict</code>.</p>
</div>
<div id="on_traintestpredict_endlogs-null" class="section level4">
<h4><code>on_(train|test|predict)_end(logs = NULL)</code></h4>
<p>Called at the end of
<code>fit</code>/<code>evaluate</code>/<code>predict</code>.</p>
</div>
</div>
<div id="batch-level-methods-for-trainingtestingpredicting" class="section level3">
<h3>Batch-level methods for training/testing/predicting</h3>
<div id="on_traintestpredict_batch_beginbatch-logs-null" class="section level4">
<h4><code>on_(train|test|predict)_batch_begin(batch, logs = NULL)</code></h4>
<p>Called right before processing a batch during
training/testing/predicting.</p>
</div>
<div id="on_traintestpredict_batch_endbatch-logs-null" class="section level4">
<h4><code>on_(train|test|predict)_batch_end(batch, logs = NULL)</code></h4>
<p>Called at the end of training/testing/predicting a batch. Within this
method, <code>logs</code> is a named list containing the metrics
results.</p>
</div>
</div>
<div id="epoch-level-methods-training-only" class="section level3">
<h3>Epoch-level methods (training only)</h3>
<div id="on_epoch_beginepoch-logs-null" class="section level4">
<h4><code>on_epoch_begin(epoch, logs = NULL)</code></h4>
<p>Called at the beginning of an epoch during training.</p>
</div>
<div id="on_epoch_endepoch-logs-null" class="section level4">
<h4><code>on_epoch_end(epoch, logs = NULL)</code></h4>
<p>Called at the end of an epoch during training.</p>
</div>
</div>
</div>
<div id="a-basic-example" class="section level2">
<h2>A basic example</h2>
<p>Let’s take a look at a concrete example. To get started, let’s import
tensorflow and define a simple Sequential Keras model:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Define the Keras model to add callbacks to</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>get_model <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>  model <span class="sc">|&gt;</span> <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>  model <span class="sc">|&gt;</span> <span class="fu">compile</span>(</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="fu">optimizer_rmsprop</span>(<span class="at">learning_rate =</span> <span class="fl">0.1</span>),</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;mean_squared_error&quot;</span>,</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="st">&quot;mean_absolute_error&quot;</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>  )</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>  model</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>}</span></code></pre></div>
<p>Then, load the MNIST data for training and testing from Keras
datasets API:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Load example MNIST data and pre-process it</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>mnist <span class="ot">&lt;-</span> <span class="fu">dataset_mnist</span>()</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>flatten_and_rescale <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(x, <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">784</span>))</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>  x <span class="ot">&lt;-</span> x <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>  x</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>}</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>mnist<span class="sc">$</span>train<span class="sc">$</span>x <span class="ot">&lt;-</span> <span class="fu">flatten_and_rescale</span>(mnist<span class="sc">$</span>train<span class="sc">$</span>x)</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>mnist<span class="sc">$</span>test<span class="sc">$</span>x  <span class="ot">&lt;-</span> <span class="fu">flatten_and_rescale</span>(mnist<span class="sc">$</span>test<span class="sc">$</span>x)</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a><span class="co"># limit to 1000 samples</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>mnist<span class="sc">$</span>train<span class="sc">$</span>x <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>x[<span class="dv">1</span><span class="sc">:</span>n,]</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>mnist<span class="sc">$</span>train<span class="sc">$</span>y <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>y[<span class="dv">1</span><span class="sc">:</span>n]</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>mnist<span class="sc">$</span>test<span class="sc">$</span>x  <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>x[<span class="dv">1</span><span class="sc">:</span>n,]</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>mnist<span class="sc">$</span>test<span class="sc">$</span>y  <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y[<span class="dv">1</span><span class="sc">:</span>n]</span></code></pre></div>
<p>Now, define a simple custom callback that logs:</p>
<ul>
<li>When <code>fit</code>/<code>evaluate</code>/<code>predict</code>
starts &amp; ends</li>
<li>When each epoch starts &amp; ends</li>
<li>When each training batch starts &amp; ends</li>
<li>When each evaluation (test) batch starts &amp; ends</li>
<li>When each inference (prediction) batch starts &amp; ends</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>show <span class="ot">&lt;-</span> <span class="cf">function</span>(msg, logs) {</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>  <span class="fu">cat</span>(glue<span class="sc">::</span><span class="fu">glue</span>(msg, <span class="at">.envir =</span> <span class="fu">parent.frame</span>()),</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>      <span class="st">&quot;got logs: &quot;</span>, <span class="at">sep =</span> <span class="st">&quot;; &quot;</span>)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>  <span class="fu">str</span>(logs); <span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>}</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>callback_custom <span class="ot">&lt;-</span> <span class="fu">Callback</span>(</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>  <span class="st">&quot;CustomCallback&quot;</span>,</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>  <span class="at">on_train_begin         =</span> \(<span class="at">logs =</span> <span class="cn">NULL</span>)        <span class="fu">show</span>(<span class="st">&quot;Starting training&quot;</span>, logs),</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>  <span class="at">on_epoch_begin         =</span> \(epoch, <span class="at">logs =</span> <span class="cn">NULL</span>) <span class="fu">show</span>(<span class="st">&quot;Start epoch {epoch} of training&quot;</span>, logs),</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>  <span class="at">on_train_batch_begin   =</span> \(batch, <span class="at">logs =</span> <span class="cn">NULL</span>) <span class="fu">show</span>(<span class="st">&quot;...Training: start of batch {batch}&quot;</span>, logs),</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>  <span class="at">on_train_batch_end     =</span> \(batch, <span class="at">logs =</span> <span class="cn">NULL</span>) <span class="fu">show</span>(<span class="st">&quot;...Training: end of batch {batch}&quot;</span>,  logs),</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>  <span class="at">on_epoch_end           =</span> \(epoch, <span class="at">logs =</span> <span class="cn">NULL</span>) <span class="fu">show</span>(<span class="st">&quot;End epoch {epoch} of training&quot;</span>, logs),</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>  <span class="at">on_train_end           =</span> \(<span class="at">logs =</span> <span class="cn">NULL</span>)        <span class="fu">show</span>(<span class="st">&quot;Stop training&quot;</span>, logs),</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>  <span class="at">on_test_begin          =</span> \(<span class="at">logs =</span> <span class="cn">NULL</span>)        <span class="fu">show</span>(<span class="st">&quot;Start testing&quot;</span>, logs),</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>  <span class="at">on_test_batch_begin    =</span> \(batch, <span class="at">logs =</span> <span class="cn">NULL</span>) <span class="fu">show</span>(<span class="st">&quot;...Evaluating: start of batch {batch}&quot;</span>, logs),</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>  <span class="at">on_test_batch_end      =</span> \(batch, <span class="at">logs =</span> <span class="cn">NULL</span>) <span class="fu">show</span>(<span class="st">&quot;...Evaluating: end of batch {batch}&quot;</span>, logs),</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>  <span class="at">on_test_end            =</span> \(<span class="at">logs =</span> <span class="cn">NULL</span>)        <span class="fu">show</span>(<span class="st">&quot;Stop testing&quot;</span>, logs),</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>  <span class="at">on_predict_begin       =</span> \(<span class="at">logs =</span> <span class="cn">NULL</span>)        <span class="fu">show</span>(<span class="st">&quot;Start predicting&quot;</span>, logs),</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>  <span class="at">on_predict_end         =</span> \(<span class="at">logs =</span> <span class="cn">NULL</span>)        <span class="fu">show</span>(<span class="st">&quot;Stop predicting&quot;</span>, logs),</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>  <span class="at">on_predict_batch_begin =</span> \(batch, <span class="at">logs =</span> <span class="cn">NULL</span>) <span class="fu">show</span>(<span class="st">&quot;...Predicting: start of batch {batch}&quot;</span>, logs),</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>  <span class="at">on_predict_batch_end   =</span> \(batch, <span class="at">logs =</span> <span class="cn">NULL</span>) <span class="fu">show</span>(<span class="st">&quot;...Predicting: end of batch {batch}&quot;</span>, logs),</span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>)</span></code></pre></div>
<p>Let’s try it out:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">get_model</span>()</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>model <span class="sc">|&gt;</span> <span class="fu">fit</span>(</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>  mnist<span class="sc">$</span>train<span class="sc">$</span>x, mnist<span class="sc">$</span>train<span class="sc">$</span>y,</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">2</span>,</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>  <span class="at">verbose =</span> <span class="dv">0</span>,</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>  <span class="at">validation_split =</span> <span class="fl">0.5</span>,</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>  <span class="at">callbacks =</span> <span class="fu">list</span>(<span class="fu">callback_custom</span>())</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Starting training; got logs:  Named list()
##
## Start epoch 1 of training; got logs:  Named list()
##
## ...Training: start of batch 1; got logs:  Named list()
##
## ...Training: end of batch 1; got logs: List of 2
##  $ loss               : num 25.9
##  $ mean_absolute_error: num 4.19
##
## ...Training: start of batch 2; got logs:  Named list()
##
## ...Training: end of batch 2; got logs: List of 2
##  $ loss               : num 433
##  $ mean_absolute_error: num 15.5
##
## ...Training: start of batch 3; got logs:  Named list()
##
## ...Training: end of batch 3; got logs: List of 2
##  $ loss               : num 297
##  $ mean_absolute_error: num 11.8
##
## ...Training: start of batch 4; got logs:  Named list()
##
## ...Training: end of batch 4; got logs: List of 2
##  $ loss               : num 231
##  $ mean_absolute_error: num 9.68
##
## Start testing; got logs:  Named list()
##
## ...Evaluating: start of batch 1; got logs:  Named list()
##
## ...Evaluating: end of batch 1; got logs: List of 2
##  $ loss               : num 8.1
##  $ mean_absolute_error: num 2.3
##
## ...Evaluating: start of batch 2; got logs:  Named list()
##
## ...Evaluating: end of batch 2; got logs: List of 2
##  $ loss               : num 7.58
##  $ mean_absolute_error: num 2.23
##
## ...Evaluating: start of batch 3; got logs:  Named list()
##
## ...Evaluating: end of batch 3; got logs: List of 2
##  $ loss               : num 7.38
##  $ mean_absolute_error: num 2.21
##
## ...Evaluating: start of batch 4; got logs:  Named list()
##
## ...Evaluating: end of batch 4; got logs: List of 2
##  $ loss               : num 7.3
##  $ mean_absolute_error: num 2.21
##
## Stop testing; got logs: List of 2
##  $ loss               : num 7.3
##  $ mean_absolute_error: num 2.21
##
## End epoch 1 of training; got logs: List of 4
##  $ loss                   : num 231
##  $ mean_absolute_error    : num 9.68
##  $ val_loss               : num 7.3
##  $ val_mean_absolute_error: num 2.21
##
## Start epoch 2 of training; got logs:  Named list()
##
## ...Training: start of batch 1; got logs:  Named list()
##
## ...Training: end of batch 1; got logs: List of 2
##  $ loss               : num 7.44
##  $ mean_absolute_error: num 2.27
##
## ...Training: start of batch 2; got logs:  Named list()
##
## ...Training: end of batch 2; got logs: List of 2
##  $ loss               : num 6.81
##  $ mean_absolute_error: num 2.16
##
## ...Training: start of batch 3; got logs:  Named list()
##
## ...Training: end of batch 3; got logs: List of 2
##  $ loss               : num 6.12
##  $ mean_absolute_error: num 2.06
##
## ...Training: start of batch 4; got logs:  Named list()
##
## ...Training: end of batch 4; got logs: List of 2
##  $ loss               : num 6.08
##  $ mean_absolute_error: num 2.04
##
## Start testing; got logs:  Named list()
##
## ...Evaluating: start of batch 1; got logs:  Named list()
##
## ...Evaluating: end of batch 1; got logs: List of 2
##  $ loss               : num 5.54
##  $ mean_absolute_error: num 1.92
##
## ...Evaluating: start of batch 2; got logs:  Named list()
##
## ...Evaluating: end of batch 2; got logs: List of 2
##  $ loss               : num 5.31
##  $ mean_absolute_error: num 1.87
##
## ...Evaluating: start of batch 3; got logs:  Named list()
##
## ...Evaluating: end of batch 3; got logs: List of 2
##  $ loss               : num 5.11
##  $ mean_absolute_error: num 1.8
##
## ...Evaluating: start of batch 4; got logs:  Named list()
##
## ...Evaluating: end of batch 4; got logs: List of 2
##  $ loss               : num 5.15
##  $ mean_absolute_error: num 1.82
##
## Stop testing; got logs: List of 2
##  $ loss               : num 5.15
##  $ mean_absolute_error: num 1.82
##
## End epoch 2 of training; got logs: List of 4
##  $ loss                   : num 6.08
##  $ mean_absolute_error    : num 2.04
##  $ val_loss               : num 5.15
##  $ val_mean_absolute_error: num 1.82
##
## Stop training; got logs: List of 4
##  $ loss                   : num 6.08
##  $ mean_absolute_error    : num 2.04
##  $ val_loss               : num 5.15
##  $ val_mean_absolute_error: num 1.82</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> model <span class="sc">|&gt;</span> <span class="fu">evaluate</span>(</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>  mnist<span class="sc">$</span>test<span class="sc">$</span>x, mnist<span class="sc">$</span>test<span class="sc">$</span>y,</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">128</span>, <span class="at">verbose =</span> <span class="dv">0</span>,</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>  <span class="at">callbacks =</span> <span class="fu">list</span>(<span class="fu">callback_custom</span>())</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Start testing; got logs:  Named list()
##
## ...Evaluating: start of batch 1; got logs:  Named list()
##
## ...Evaluating: end of batch 1; got logs: List of 2
##  $ loss               : num 5.2
##  $ mean_absolute_error: num 1.84
##
## ...Evaluating: start of batch 2; got logs:  Named list()
##
## ...Evaluating: end of batch 2; got logs: List of 2
##  $ loss               : num 4.62
##  $ mean_absolute_error: num 1.73
##
## ...Evaluating: start of batch 3; got logs:  Named list()
##
## ...Evaluating: end of batch 3; got logs: List of 2
##  $ loss               : num 4.61
##  $ mean_absolute_error: num 1.74
##
## ...Evaluating: start of batch 4; got logs:  Named list()
##
## ...Evaluating: end of batch 4; got logs: List of 2
##  $ loss               : num 4.65
##  $ mean_absolute_error: num 1.75
##
## ...Evaluating: start of batch 5; got logs:  Named list()
##
## ...Evaluating: end of batch 5; got logs: List of 2
##  $ loss               : num 4.84
##  $ mean_absolute_error: num 1.77
##
## ...Evaluating: start of batch 6; got logs:  Named list()
##
## ...Evaluating: end of batch 6; got logs: List of 2
##  $ loss               : num 4.76
##  $ mean_absolute_error: num 1.76
##
## ...Evaluating: start of batch 7; got logs:  Named list()
##
## ...Evaluating: end of batch 7; got logs: List of 2
##  $ loss               : num 4.74
##  $ mean_absolute_error: num 1.76
##
## ...Evaluating: start of batch 8; got logs:  Named list()
##
## ...Evaluating: end of batch 8; got logs: List of 2
##  $ loss               : num 4.67
##  $ mean_absolute_error: num 1.75
##
## Stop testing; got logs: List of 2
##  $ loss               : num 4.67
##  $ mean_absolute_error: num 1.75</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> model <span class="sc">|&gt;</span> <span class="fu">predict</span>(</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>  mnist<span class="sc">$</span>test<span class="sc">$</span>x,</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">128</span>, <span class="at">verbose =</span> <span class="dv">0</span>,</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>  <span class="at">callbacks =</span> <span class="fu">list</span>(<span class="fu">callback_custom</span>())</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Start predicting; got logs:  Named list()
##
## ...Predicting: start of batch 1; got logs:  Named list()
##
## ...Predicting: end of batch 1; got logs: List of 1
##  $ outputs:&lt;tf.Tensor: shape=(128, 1), dtype=float32, numpy=…&gt;
##
## ...Predicting: start of batch 2; got logs:  Named list()
##
## ...Predicting: end of batch 2; got logs: List of 1
##  $ outputs:&lt;tf.Tensor: shape=(128, 1), dtype=float32, numpy=…&gt;
##
## ...Predicting: start of batch 3; got logs:  Named list()
##
## ...Predicting: end of batch 3; got logs: List of 1
##  $ outputs:&lt;tf.Tensor: shape=(128, 1), dtype=float32, numpy=…&gt;
##
## ...Predicting: start of batch 4; got logs:  Named list()
##
## ...Predicting: end of batch 4; got logs: List of 1
##  $ outputs:&lt;tf.Tensor: shape=(128, 1), dtype=float32, numpy=…&gt;
##
## ...Predicting: start of batch 5; got logs:  Named list()
##
## ...Predicting: end of batch 5; got logs: List of 1
##  $ outputs:&lt;tf.Tensor: shape=(128, 1), dtype=float32, numpy=…&gt;
##
## ...Predicting: start of batch 6; got logs:  Named list()
##
## ...Predicting: end of batch 6; got logs: List of 1
##  $ outputs:&lt;tf.Tensor: shape=(128, 1), dtype=float32, numpy=…&gt;
##
## ...Predicting: start of batch 7; got logs:  Named list()
##
## ...Predicting: end of batch 7; got logs: List of 1
##  $ outputs:&lt;tf.Tensor: shape=(128, 1), dtype=float32, numpy=…&gt;
##
## ...Predicting: start of batch 8; got logs:  Named list()
##
## ...Predicting: end of batch 8; got logs: List of 1
##  $ outputs:&lt;tf.Tensor: shape=(104, 1), dtype=float32, numpy=…&gt;
##
## Stop predicting; got logs:  Named list()</code></pre>
<div id="usage-of-logs-list" class="section level3">
<h3>Usage of <code>logs</code> list</h3>
<p>The <code>logs</code> named list contains the loss value, and all the
metrics at the end of a batch or epoch. Example includes the loss and
mean absolute error.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>callback_print_loss_and_mae <span class="ot">&lt;-</span> <span class="fu">Callback</span>(</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>  <span class="st">&quot;LossAndErrorPrintingCallback&quot;</span>,</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>  <span class="at">on_train_batch_end =</span> <span class="cf">function</span>(batch, <span class="at">logs =</span> <span class="cn">NULL</span>)</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Up to batch %i, the average loss is %7.2f.</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>                batch,  logs<span class="sc">$</span>loss)),</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>  <span class="at">on_test_batch_end =</span> <span class="cf">function</span>(batch, <span class="at">logs =</span> <span class="cn">NULL</span>)</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Up to batch %i, the average loss is %7.2f.</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>                batch, logs<span class="sc">$</span>loss)),</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>  <span class="at">on_epoch_end =</span> <span class="cf">function</span>(epoch, <span class="at">logs =</span> <span class="cn">NULL</span>)</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>      <span class="st">&quot;The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>      epoch, logs<span class="sc">$</span>loss, logs<span class="sc">$</span>mean_absolute_error</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a>    ))</span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a>)</span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">get_model</span>()</span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a>model <span class="sc">|&gt;</span> <span class="fu">fit</span>(</span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a>  mnist<span class="sc">$</span>train<span class="sc">$</span>x, mnist<span class="sc">$</span>train<span class="sc">$</span>y,</span>
<span id="cb11-23"><a href="#cb11-23" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">2</span>, <span class="at">verbose =</span> <span class="dv">0</span>, <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb11-24"><a href="#cb11-24" tabindex="-1"></a>  <span class="at">callbacks =</span> <span class="fu">list</span>(<span class="fu">callback_print_loss_and_mae</span>())</span>
<span id="cb11-25"><a href="#cb11-25" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Up to batch 1, the average loss is   25.12.
## Up to batch 2, the average loss is  398.92.
## Up to batch 3, the average loss is  274.04.
## Up to batch 4, the average loss is  208.32.
## Up to batch 5, the average loss is  168.15.
## Up to batch 6, the average loss is  141.31.
## Up to batch 7, the average loss is  122.19.
## Up to batch 8, the average loss is  110.05.
## The average loss for epoch  1 is    110.05 and mean absolute error is    5.79.
## Up to batch 1, the average loss is    4.71.
## Up to batch 2, the average loss is    4.74.
## Up to batch 3, the average loss is    4.81.
## Up to batch 4, the average loss is    5.07.
## Up to batch 5, the average loss is    5.08.
## Up to batch 6, the average loss is    5.09.
## Up to batch 7, the average loss is    5.19.
## Up to batch 8, the average loss is    5.51.
## The average loss for epoch  2 is      5.51 and mean absolute error is    1.90.</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>res <span class="ot">=</span> model <span class="sc">|&gt;</span> <span class="fu">evaluate</span>(</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>  mnist<span class="sc">$</span>test<span class="sc">$</span>x, mnist<span class="sc">$</span>test<span class="sc">$</span>y,</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>  <span class="at">verbose =</span> <span class="dv">0</span>, <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>  <span class="at">callbacks =</span> <span class="fu">list</span>(<span class="fu">callback_print_loss_and_mae</span>())</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Up to batch 1, the average loss is   15.86.
## Up to batch 2, the average loss is   16.13.
## Up to batch 3, the average loss is   16.02.
## Up to batch 4, the average loss is   16.11.
## Up to batch 5, the average loss is   16.23.
## Up to batch 6, the average loss is   16.68.
## Up to batch 7, the average loss is   16.61.
## Up to batch 8, the average loss is   16.54.</code></pre>
<p>For more information about callbacks, you can check out the <a href="https://keras.posit.co/reference/index.html#callbacks">Keras
callback API documentation</a>.</p>
</div>
</div>
<div id="usage-of-selfmodel-attribute" class="section level2">
<h2>Usage of <code>self$model</code> attribute</h2>
<p>In addition to receiving log information when one of their methods is
called, callbacks have access to the model associated with the current
round of training/evaluation/inference: <code>self$model</code>.</p>
<p>Here are of few of the things you can do with <code>self$model</code>
in a callback:</p>
<ul>
<li>Set <code>self$model$stop_training &lt;- TRUE</code> to immediately
interrupt training.</li>
<li>Mutate hyperparameters of the optimizer (available as
<code>self$model$optimizer</code>), such as
<code>self$model$optimizer$learning_rate</code>.</li>
<li>Save the model at period intervals.</li>
<li>Record the output of <code>model |&gt; predict()</code> on a few
test samples at the end of each epoch, to use as a sanity check during
training.</li>
<li>Extract visualizations of intermediate features at the end of each
epoch, to monitor what the model is learning over time.</li>
<li>etc.</li>
</ul>
<p>Let’s see this in action in a couple of examples.</p>
</div>
<div id="examples-of-keras-callback-applications" class="section level2">
<h2>Examples of Keras callback applications</h2>
<div id="early-stopping-at-minimum-loss" class="section level3">
<h3>Early stopping at minimum loss</h3>
<p>This first example shows the creation of a <code>Callback</code> that
stops training when the minimum of loss has been reached, by setting the
attribute <code>self$model$stop_training</code> (boolean). Optionally,
you can provide an argument <code>patience</code> to specify how many
epochs we should wait before stopping after having reached a local
minimum.</p>
<p><code>callback_early_stopping()</code> provides a more complete and
general implementation.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>callback_early_stopping_at_min_loss <span class="ot">&lt;-</span> <span class="fu">Callback</span>(</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>  <span class="st">&quot;EarlyStoppingAtMinLoss&quot;</span>,</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>  <span class="st">`</span><span class="at">__doc__</span><span class="st">`</span> <span class="ot">=</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>    <span class="st">&quot;Stop training when the loss is at its min, i.e. the loss stops decreasing.</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="st">    Arguments:</span></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="st">        patience: Number of epochs to wait after min has been hit. After this</span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a><span class="st">        number of no improvement, training stops.</span></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a><span class="st">    &quot;</span>,</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(<span class="at">patience =</span> <span class="dv">0</span>) {</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>()</span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>    self<span class="sc">$</span>patience <span class="ot">&lt;-</span> patience</span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a>    <span class="co"># best_weights to store the weights at which the minimum loss occurs.</span></span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a>    self<span class="sc">$</span>best_weights <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a>  },</span>
<span id="cb15-17"><a href="#cb15-17" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" tabindex="-1"></a>  <span class="at">on_train_begin =</span> <span class="cf">function</span>(<span class="at">logs =</span> <span class="cn">NULL</span>) {</span>
<span id="cb15-19"><a href="#cb15-19" tabindex="-1"></a>    <span class="co"># The number of epoch it has waited when loss is no longer minimum.</span></span>
<span id="cb15-20"><a href="#cb15-20" tabindex="-1"></a>    self<span class="sc">$</span>wait <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb15-21"><a href="#cb15-21" tabindex="-1"></a>    <span class="co"># The epoch the training stops at.</span></span>
<span id="cb15-22"><a href="#cb15-22" tabindex="-1"></a>    self<span class="sc">$</span>stopped_epoch <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb15-23"><a href="#cb15-23" tabindex="-1"></a>    <span class="co"># Initialize the best as infinity.</span></span>
<span id="cb15-24"><a href="#cb15-24" tabindex="-1"></a>    self<span class="sc">$</span>best <span class="ot">&lt;-</span> <span class="cn">Inf</span></span>
<span id="cb15-25"><a href="#cb15-25" tabindex="-1"></a>  },</span>
<span id="cb15-26"><a href="#cb15-26" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" tabindex="-1"></a>  <span class="at">on_epoch_end =</span> <span class="cf">function</span>(epoch, <span class="at">logs =</span> <span class="cn">NULL</span>) {</span>
<span id="cb15-28"><a href="#cb15-28" tabindex="-1"></a>    current <span class="ot">&lt;-</span> logs<span class="sc">$</span>loss</span>
<span id="cb15-29"><a href="#cb15-29" tabindex="-1"></a>    <span class="cf">if</span> (current <span class="sc">&lt;</span> self<span class="sc">$</span>best) {</span>
<span id="cb15-30"><a href="#cb15-30" tabindex="-1"></a>      self<span class="sc">$</span>best <span class="ot">&lt;-</span> current</span>
<span id="cb15-31"><a href="#cb15-31" tabindex="-1"></a>      self<span class="sc">$</span>wait <span class="ot">&lt;-</span> <span class="dv">0</span><span class="dt">L</span></span>
<span id="cb15-32"><a href="#cb15-32" tabindex="-1"></a>      <span class="co"># Record the best weights if current results is better (less).</span></span>
<span id="cb15-33"><a href="#cb15-33" tabindex="-1"></a>      self<span class="sc">$</span>best_weights <span class="ot">&lt;-</span> <span class="fu">get_weights</span>(self<span class="sc">$</span>model)</span>
<span id="cb15-34"><a href="#cb15-34" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb15-35"><a href="#cb15-35" tabindex="-1"></a>      <span class="fu">add</span>(self<span class="sc">$</span>wait) <span class="ot">&lt;-</span> <span class="dv">1</span><span class="dt">L</span></span>
<span id="cb15-36"><a href="#cb15-36" tabindex="-1"></a>      <span class="cf">if</span> (self<span class="sc">$</span>wait <span class="sc">&gt;=</span> self<span class="sc">$</span>patience) {</span>
<span id="cb15-37"><a href="#cb15-37" tabindex="-1"></a>        self<span class="sc">$</span>stopped_epoch <span class="ot">&lt;-</span> epoch</span>
<span id="cb15-38"><a href="#cb15-38" tabindex="-1"></a>        self<span class="sc">$</span>model<span class="sc">$</span>stop_training <span class="ot">&lt;-</span> <span class="cn">TRUE</span></span>
<span id="cb15-39"><a href="#cb15-39" tabindex="-1"></a>        <span class="fu">cat</span>(<span class="st">&quot;Restoring model weights from the end of the best epoch.</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb15-40"><a href="#cb15-40" tabindex="-1"></a>        model<span class="sc">$</span><span class="fu">set_weights</span>(self<span class="sc">$</span>best_weights)</span>
<span id="cb15-41"><a href="#cb15-41" tabindex="-1"></a>      }</span>
<span id="cb15-42"><a href="#cb15-42" tabindex="-1"></a>    }</span>
<span id="cb15-43"><a href="#cb15-43" tabindex="-1"></a>  },</span>
<span id="cb15-44"><a href="#cb15-44" tabindex="-1"></a></span>
<span id="cb15-45"><a href="#cb15-45" tabindex="-1"></a>  <span class="at">on_train_end =</span> <span class="cf">function</span>(<span class="at">logs =</span> <span class="cn">NULL</span>)</span>
<span id="cb15-46"><a href="#cb15-46" tabindex="-1"></a>    <span class="cf">if</span> (self<span class="sc">$</span>stopped_epoch <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb15-47"><a href="#cb15-47" tabindex="-1"></a>      <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Epoch %05d: early stopping</span><span class="sc">\n</span><span class="st">&quot;</span>, self<span class="sc">$</span>stopped_epoch <span class="sc">+</span> <span class="dv">1</span>))</span>
<span id="cb15-48"><a href="#cb15-48" tabindex="-1"></a>)</span>
<span id="cb15-49"><a href="#cb15-49" tabindex="-1"></a><span class="st">`</span><span class="at">add&lt;-</span><span class="st">`</span> <span class="ot">&lt;-</span> <span class="st">`</span><span class="at">+</span><span class="st">`</span></span>
<span id="cb15-50"><a href="#cb15-50" tabindex="-1"></a></span>
<span id="cb15-51"><a href="#cb15-51" tabindex="-1"></a></span>
<span id="cb15-52"><a href="#cb15-52" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">get_model</span>()</span>
<span id="cb15-53"><a href="#cb15-53" tabindex="-1"></a>model <span class="sc">|&gt;</span> <span class="fu">fit</span>(</span>
<span id="cb15-54"><a href="#cb15-54" tabindex="-1"></a>  mnist<span class="sc">$</span>train<span class="sc">$</span>x,</span>
<span id="cb15-55"><a href="#cb15-55" tabindex="-1"></a>  mnist<span class="sc">$</span>train<span class="sc">$</span>y,</span>
<span id="cb15-56"><a href="#cb15-56" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">30</span>,</span>
<span id="cb15-57"><a href="#cb15-57" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">64</span>,</span>
<span id="cb15-58"><a href="#cb15-58" tabindex="-1"></a>  <span class="at">verbose =</span> <span class="dv">0</span>,</span>
<span id="cb15-59"><a href="#cb15-59" tabindex="-1"></a>  <span class="at">callbacks =</span> <span class="fu">list</span>(<span class="fu">callback_print_loss_and_mae</span>(),</span>
<span id="cb15-60"><a href="#cb15-60" tabindex="-1"></a>                   <span class="fu">callback_early_stopping_at_min_loss</span>())</span>
<span id="cb15-61"><a href="#cb15-61" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Up to batch 1, the average loss is   30.54.
## Up to batch 2, the average loss is  513.27.
## Up to batch 3, the average loss is  352.60.
## Up to batch 4, the average loss is  266.37.
## Up to batch 5, the average loss is  214.68.
## Up to batch 6, the average loss is  179.97.
## Up to batch 7, the average loss is  155.06.
## Up to batch 8, the average loss is  136.59.
## Up to batch 9, the average loss is  121.96.
## Up to batch 10, the average loss is  110.28.
## Up to batch 11, the average loss is  100.72.
## Up to batch 12, the average loss is   92.71.
## Up to batch 13, the average loss is   85.95.
## Up to batch 14, the average loss is   80.21.
## Up to batch 15, the average loss is   75.17.
## Up to batch 16, the average loss is   72.48.
## The average loss for epoch  1 is     72.48 and mean absolute error is    4.08.
## Up to batch 1, the average loss is    7.98.
## Up to batch 2, the average loss is    9.92.
## Up to batch 3, the average loss is   12.88.
## Up to batch 4, the average loss is   16.61.
## Up to batch 5, the average loss is   20.49.
## Up to batch 6, the average loss is   26.14.
## Up to batch 7, the average loss is   30.44.
## Up to batch 8, the average loss is   33.76.
## Up to batch 9, the average loss is   36.32.
## Up to batch 10, the average loss is   35.26.
## Up to batch 11, the average loss is   34.22.
## Up to batch 12, the average loss is   33.53.
## Up to batch 13, the average loss is   32.84.
## Up to batch 14, the average loss is   31.80.
## Up to batch 15, the average loss is   31.39.
## Up to batch 16, the average loss is   31.45.
## The average loss for epoch  2 is     31.45 and mean absolute error is    4.82.
## Up to batch 1, the average loss is   39.60.
## Up to batch 2, the average loss is   41.95.
## Up to batch 3, the average loss is   41.29.
## Up to batch 4, the average loss is   36.77.
## Up to batch 5, the average loss is   32.08.
## Up to batch 6, the average loss is   28.17.
## Up to batch 7, the average loss is   25.33.
## Up to batch 8, the average loss is   23.56.
## Up to batch 9, the average loss is   22.28.
## Up to batch 10, the average loss is   21.22.
## Up to batch 11, the average loss is   20.87.
## Up to batch 12, the average loss is   22.25.
## Up to batch 13, the average loss is   25.08.
## Up to batch 14, the average loss is   27.87.
## Up to batch 15, the average loss is   31.72.
## Up to batch 16, the average loss is   33.21.
## The average loss for epoch  3 is     33.21 and mean absolute error is    4.79.
## Restoring model weights from the end of the best epoch.
## Epoch 00004: early stopping</code></pre>
</div>
<div id="learning-rate-scheduling" class="section level3">
<h3>Learning rate scheduling</h3>
<p>In this example, we show how a custom Callback can be used to
dynamically change the learning rate of the optimizer during the course
of training.</p>
<p>See <code>keras$callbacks$LearningRateScheduler</code> for a more
general implementations (in RStudio, press F1 while the cursor is over
<code>LearningRateScheduler</code> and a browser will open to <a href="https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/keras/callbacks/LearningRateScheduler">this
page</a>).</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>callback_custom_learning_rate_scheduler <span class="ot">&lt;-</span> <span class="fu">Callback</span>(</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>  <span class="st">&quot;CustomLearningRateScheduler&quot;</span>,</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>  <span class="st">`</span><span class="at">__doc__</span><span class="st">`</span> <span class="ot">=</span></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>  <span class="st">&quot;Learning rate scheduler which sets the learning rate according to schedule.</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a><span class="st">    Arguments:</span></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a><span class="st">        schedule: a function that takes an epoch index</span></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a><span class="st">            (integer, indexed from 0) and current learning rate</span></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a><span class="st">            as inputs and returns a new learning rate as output (float).</span></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a><span class="st">    &quot;</span>,</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(schedule) {</span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>()</span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a>    self<span class="sc">$</span>schedule <span class="ot">&lt;-</span> schedule</span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a>  },</span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a>  <span class="at">on_epoch_begin =</span> <span class="cf">function</span>(epoch, <span class="at">logs =</span> <span class="cn">NULL</span>) {</span>
<span id="cb17-18"><a href="#cb17-18" tabindex="-1"></a>    <span class="do">## When in doubt about what types of objects are in scope (e.g., self$model)</span></span>
<span id="cb17-19"><a href="#cb17-19" tabindex="-1"></a>    <span class="do">## use a debugger to interact with the actual objects at the console!</span></span>
<span id="cb17-20"><a href="#cb17-20" tabindex="-1"></a>    <span class="co"># browser()</span></span>
<span id="cb17-21"><a href="#cb17-21" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="st">&quot;learning_rate&quot;</span> <span class="sc">%in%</span> <span class="fu">names</span>(self<span class="sc">$</span>model<span class="sc">$</span>optimizer))</span>
<span id="cb17-23"><a href="#cb17-23" tabindex="-1"></a>      <span class="fu">stop</span>(<span class="st">&#39;Optimizer must have a &quot;learning_rate&quot; attribute.&#39;</span>)</span>
<span id="cb17-24"><a href="#cb17-24" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" tabindex="-1"></a>    <span class="co"># # Get the current learning rate from model&#39;s optimizer.</span></span>
<span id="cb17-26"><a href="#cb17-26" tabindex="-1"></a>    <span class="co"># use as.numeric() to convert the keras variablea to an R numeric</span></span>
<span id="cb17-27"><a href="#cb17-27" tabindex="-1"></a>    lr <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(self<span class="sc">$</span>model<span class="sc">$</span>optimizer<span class="sc">$</span>learning_rate)</span>
<span id="cb17-28"><a href="#cb17-28" tabindex="-1"></a>    <span class="co"># # Call schedule function to get the scheduled learning rate.</span></span>
<span id="cb17-29"><a href="#cb17-29" tabindex="-1"></a>    scheduled_lr <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">schedule</span>(epoch, lr)</span>
<span id="cb17-30"><a href="#cb17-30" tabindex="-1"></a>    <span class="co"># # Set the value back to the optimizer before this epoch starts</span></span>
<span id="cb17-31"><a href="#cb17-31" tabindex="-1"></a>    optimizer <span class="ot">&lt;-</span> self<span class="sc">$</span>model<span class="sc">$</span>optimizer</span>
<span id="cb17-32"><a href="#cb17-32" tabindex="-1"></a>    optimizer<span class="sc">$</span>learning_rate <span class="ot">&lt;-</span> scheduled_lr</span>
<span id="cb17-33"><a href="#cb17-33" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Epoch %03d: Learning rate is %6.4f.</span><span class="sc">\n</span><span class="st">&quot;</span>, epoch, scheduled_lr))</span>
<span id="cb17-34"><a href="#cb17-34" tabindex="-1"></a>  }</span>
<span id="cb17-35"><a href="#cb17-35" tabindex="-1"></a>)</span>
<span id="cb17-36"><a href="#cb17-36" tabindex="-1"></a></span>
<span id="cb17-37"><a href="#cb17-37" tabindex="-1"></a>LR_SCHEDULE <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tribble</span>(</span>
<span id="cb17-38"><a href="#cb17-38" tabindex="-1"></a>  <span class="sc">~</span>start_epoch, <span class="sc">~</span>learning_rate,</span>
<span id="cb17-39"><a href="#cb17-39" tabindex="-1"></a>             <span class="dv">0</span>,            <span class="fl">0.1</span>,</span>
<span id="cb17-40"><a href="#cb17-40" tabindex="-1"></a>             <span class="dv">3</span>,           <span class="fl">0.05</span>,</span>
<span id="cb17-41"><a href="#cb17-41" tabindex="-1"></a>             <span class="dv">6</span>,           <span class="fl">0.01</span>,</span>
<span id="cb17-42"><a href="#cb17-42" tabindex="-1"></a>             <span class="dv">9</span>,          <span class="fl">0.005</span>,</span>
<span id="cb17-43"><a href="#cb17-43" tabindex="-1"></a>            <span class="dv">12</span>,          <span class="fl">0.001</span>,</span>
<span id="cb17-44"><a href="#cb17-44" tabindex="-1"></a>  )</span>
<span id="cb17-45"><a href="#cb17-45" tabindex="-1"></a></span>
<span id="cb17-46"><a href="#cb17-46" tabindex="-1"></a>last <span class="ot">&lt;-</span> <span class="cf">function</span>(x) x[<span class="fu">length</span>(x)]</span>
<span id="cb17-47"><a href="#cb17-47" tabindex="-1"></a>lr_schedule <span class="ot">&lt;-</span> <span class="cf">function</span>(epoch, learning_rate) {</span>
<span id="cb17-48"><a href="#cb17-48" tabindex="-1"></a>  <span class="st">&quot;Helper function to retrieve the scheduled learning rate based on epoch.&quot;</span></span>
<span id="cb17-49"><a href="#cb17-49" tabindex="-1"></a>  <span class="fu">with</span>(LR_SCHEDULE, learning_rate[<span class="fu">last</span>(<span class="fu">which</span>(epoch <span class="sc">&gt;=</span> start_epoch))])</span>
<span id="cb17-50"><a href="#cb17-50" tabindex="-1"></a>}</span>
<span id="cb17-51"><a href="#cb17-51" tabindex="-1"></a></span>
<span id="cb17-52"><a href="#cb17-52" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">get_model</span>()</span>
<span id="cb17-53"><a href="#cb17-53" tabindex="-1"></a>model <span class="sc">|&gt;</span> <span class="fu">fit</span>(</span>
<span id="cb17-54"><a href="#cb17-54" tabindex="-1"></a>  mnist<span class="sc">$</span>train<span class="sc">$</span>x,</span>
<span id="cb17-55"><a href="#cb17-55" tabindex="-1"></a>  mnist<span class="sc">$</span>train<span class="sc">$</span>y,</span>
<span id="cb17-56"><a href="#cb17-56" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">14</span>,</span>
<span id="cb17-57"><a href="#cb17-57" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">64</span>,</span>
<span id="cb17-58"><a href="#cb17-58" tabindex="-1"></a>  <span class="at">verbose =</span> <span class="dv">0</span>,</span>
<span id="cb17-59"><a href="#cb17-59" tabindex="-1"></a>  <span class="at">callbacks =</span> <span class="fu">list</span>(</span>
<span id="cb17-60"><a href="#cb17-60" tabindex="-1"></a>    <span class="fu">callback_print_loss_and_mae</span>(),</span>
<span id="cb17-61"><a href="#cb17-61" tabindex="-1"></a>    <span class="fu">callback_custom_learning_rate_scheduler</span>(lr_schedule)</span>
<span id="cb17-62"><a href="#cb17-62" tabindex="-1"></a>  )</span>
<span id="cb17-63"><a href="#cb17-63" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>##
## Epoch 001: Learning rate is 0.1000.
## Up to batch 1, the average loss is   29.36.
## Up to batch 2, the average loss is  513.95.
## Up to batch 3, the average loss is  352.70.
## Up to batch 4, the average loss is  266.46.
## Up to batch 5, the average loss is  214.73.
## Up to batch 6, the average loss is  180.00.
## Up to batch 7, the average loss is  155.05.
## Up to batch 8, the average loss is  136.64.
## Up to batch 9, the average loss is  121.97.
## Up to batch 10, the average loss is  110.30.
## Up to batch 11, the average loss is  100.76.
## Up to batch 12, the average loss is   92.74.
## Up to batch 13, the average loss is   85.95.
## Up to batch 14, the average loss is   80.18.
## Up to batch 15, the average loss is   75.11.
## Up to batch 16, the average loss is   72.38.
## The average loss for epoch  1 is     72.38 and mean absolute error is    4.04.
##
## Epoch 002: Learning rate is 0.1000.
## Up to batch 1, the average loss is    6.95.
## Up to batch 2, the average loss is    8.71.
## Up to batch 3, the average loss is   11.42.
## Up to batch 4, the average loss is   15.15.
## Up to batch 5, the average loss is   19.28.
## Up to batch 6, the average loss is   25.54.
## Up to batch 7, the average loss is   30.38.
## Up to batch 8, the average loss is   33.95.
## Up to batch 9, the average loss is   36.58.
## Up to batch 10, the average loss is   35.46.
## Up to batch 11, the average loss is   34.34.
## Up to batch 12, the average loss is   33.51.
## Up to batch 13, the average loss is   32.67.
## Up to batch 14, the average loss is   31.54.
## Up to batch 15, the average loss is   31.05.
## Up to batch 16, the average loss is   31.09.
## The average loss for epoch  2 is     31.09 and mean absolute error is    4.77.
##
## Epoch 003: Learning rate is 0.0500.
## Up to batch 1, the average loss is   40.40.
## Up to batch 2, the average loss is   22.33.
## Up to batch 3, the average loss is   16.18.
## Up to batch 4, the average loss is   13.09.
## Up to batch 5, the average loss is   11.48.
## Up to batch 6, the average loss is   10.21.
## Up to batch 7, the average loss is    9.22.
## Up to batch 8, the average loss is    8.70.
## Up to batch 9, the average loss is    8.16.
## Up to batch 10, the average loss is    7.80.
## Up to batch 11, the average loss is    7.50.
## Up to batch 12, the average loss is    7.17.
## Up to batch 13, the average loss is    6.89.
## Up to batch 14, the average loss is    6.70.
## Up to batch 15, the average loss is    6.52.
## Up to batch 16, the average loss is    6.54.
## The average loss for epoch  3 is      6.54 and mean absolute error is    1.93.
##
## Epoch 004: Learning rate is 0.0500.
## Up to batch 1, the average loss is    8.74.
## Up to batch 2, the average loss is    8.34.
## Up to batch 3, the average loss is    9.09.
## Up to batch 4, the average loss is    9.72.
## Up to batch 5, the average loss is   10.48.
## Up to batch 6, the average loss is   11.69.
## Up to batch 7, the average loss is   11.83.
## Up to batch 8, the average loss is   11.56.
## Up to batch 9, the average loss is   11.24.
## Up to batch 10, the average loss is   10.84.
## Up to batch 11, the average loss is   10.66.
## Up to batch 12, the average loss is   10.44.
## Up to batch 13, the average loss is   10.21.
## Up to batch 14, the average loss is   10.06.
## Up to batch 15, the average loss is   10.00.
## Up to batch 16, the average loss is   10.20.
## The average loss for epoch  4 is     10.20 and mean absolute error is    2.71.
##
## Epoch 005: Learning rate is 0.0500.
## Up to batch 1, the average loss is   17.26.
## Up to batch 2, the average loss is   14.09.
## Up to batch 3, the average loss is   12.67.
## Up to batch 4, the average loss is   11.44.
## Up to batch 5, the average loss is   10.54.
## Up to batch 6, the average loss is   10.10.
## Up to batch 7, the average loss is    9.53.
## Up to batch 8, the average loss is    9.17.
## Up to batch 9, the average loss is    8.78.
## Up to batch 10, the average loss is    8.49.
## Up to batch 11, the average loss is    8.50.
## Up to batch 12, the average loss is    8.59.
## Up to batch 13, the average loss is    8.68.
## Up to batch 14, the average loss is    8.86.
## Up to batch 15, the average loss is    9.17.
## Up to batch 16, the average loss is    9.53.
## The average loss for epoch  5 is      9.53 and mean absolute error is    2.58.
##
## Epoch 006: Learning rate is 0.0100.
## Up to batch 1, the average loss is   17.04.
## Up to batch 2, the average loss is   14.85.
## Up to batch 3, the average loss is   11.53.
## Up to batch 4, the average loss is    9.65.
## Up to batch 5, the average loss is    8.44.
## Up to batch 6, the average loss is    7.50.
## Up to batch 7, the average loss is    6.74.
## Up to batch 8, the average loss is    6.56.
## Up to batch 9, the average loss is    6.18.
## Up to batch 10, the average loss is    5.87.
## Up to batch 11, the average loss is    5.63.
## Up to batch 12, the average loss is    5.45.
## Up to batch 13, the average loss is    5.23.
## Up to batch 14, the average loss is    5.12.
## Up to batch 15, the average loss is    4.96.
## Up to batch 16, the average loss is    4.91.
## The average loss for epoch  6 is      4.91 and mean absolute error is    1.67.
##
## Epoch 007: Learning rate is 0.0100.
## Up to batch 1, the average loss is    3.65.
## Up to batch 2, the average loss is    3.04.
## Up to batch 3, the average loss is    2.88.
## Up to batch 4, the average loss is    2.85.
## Up to batch 5, the average loss is    2.88.
## Up to batch 6, the average loss is    2.81.
## Up to batch 7, the average loss is    2.70.
## Up to batch 8, the average loss is    2.96.
## Up to batch 9, the average loss is    2.96.
## Up to batch 10, the average loss is    2.93.
## Up to batch 11, the average loss is    2.95.
## Up to batch 12, the average loss is    2.98.
## Up to batch 13, the average loss is    2.97.
## Up to batch 14, the average loss is    3.01.
## Up to batch 15, the average loss is    3.00.
## Up to batch 16, the average loss is    3.05.
## The average loss for epoch  7 is      3.05 and mean absolute error is    1.34.
##
## Epoch 008: Learning rate is 0.0100.
## Up to batch 1, the average loss is    3.69.
## Up to batch 2, the average loss is    3.21.
## Up to batch 3, the average loss is    3.00.
## Up to batch 4, the average loss is    2.91.
## Up to batch 5, the average loss is    2.94.
## Up to batch 6, the average loss is    2.85.
## Up to batch 7, the average loss is    2.72.
## Up to batch 8, the average loss is    2.95.
## Up to batch 9, the average loss is    2.97.
## Up to batch 10, the average loss is    2.93.
## Up to batch 11, the average loss is    2.96.
## Up to batch 12, the average loss is    2.98.
## Up to batch 13, the average loss is    2.99.
## Up to batch 14, the average loss is    3.05.
## Up to batch 15, the average loss is    3.08.
## Up to batch 16, the average loss is    3.14.
## The average loss for epoch  8 is      3.14 and mean absolute error is    1.36.
##
## Epoch 009: Learning rate is 0.0050.
## Up to batch 1, the average loss is    3.71.
## Up to batch 2, the average loss is    2.93.
## Up to batch 3, the average loss is    2.76.
## Up to batch 4, the average loss is    2.70.
## Up to batch 5, the average loss is    2.76.
## Up to batch 6, the average loss is    2.69.
## Up to batch 7, the average loss is    2.57.
## Up to batch 8, the average loss is    2.79.
## Up to batch 9, the average loss is    2.80.
## Up to batch 10, the average loss is    2.77.
## Up to batch 11, the average loss is    2.79.
## Up to batch 12, the average loss is    2.80.
## Up to batch 13, the average loss is    2.78.
## Up to batch 14, the average loss is    2.81.
## Up to batch 15, the average loss is    2.80.
## Up to batch 16, the average loss is    2.83.
## The average loss for epoch  9 is      2.83 and mean absolute error is    1.28.
##
## Epoch 010: Learning rate is 0.0050.
## Up to batch 1, the average loss is    3.02.
## Up to batch 2, the average loss is    2.69.
## Up to batch 3, the average loss is    2.58.
## Up to batch 4, the average loss is    2.57.
## Up to batch 5, the average loss is    2.65.
## Up to batch 6, the average loss is    2.60.
## Up to batch 7, the average loss is    2.48.
## Up to batch 8, the average loss is    2.72.
## Up to batch 9, the average loss is    2.74.
## Up to batch 10, the average loss is    2.71.
## Up to batch 11, the average loss is    2.74.
## Up to batch 12, the average loss is    2.75.
## Up to batch 13, the average loss is    2.74.
## Up to batch 14, the average loss is    2.77.
## Up to batch 15, the average loss is    2.77.
## Up to batch 16, the average loss is    2.80.
## The average loss for epoch 10 is      2.80 and mean absolute error is    1.27.
##
## Epoch 011: Learning rate is 0.0050.
## Up to batch 1, the average loss is    3.01.
## Up to batch 2, the average loss is    2.69.
## Up to batch 3, the average loss is    2.58.
## Up to batch 4, the average loss is    2.56.
## Up to batch 5, the average loss is    2.63.
## Up to batch 6, the average loss is    2.58.
## Up to batch 7, the average loss is    2.47.
## Up to batch 8, the average loss is    2.70.
## Up to batch 9, the average loss is    2.72.
## Up to batch 10, the average loss is    2.69.
## Up to batch 11, the average loss is    2.71.
## Up to batch 12, the average loss is    2.72.
## Up to batch 13, the average loss is    2.71.
## Up to batch 14, the average loss is    2.75.
## Up to batch 15, the average loss is    2.74.
## Up to batch 16, the average loss is    2.77.
## The average loss for epoch 11 is      2.77 and mean absolute error is    1.27.
##
## Epoch 012: Learning rate is 0.0010.
## Up to batch 1, the average loss is    2.96.
## Up to batch 2, the average loss is    2.53.
## Up to batch 3, the average loss is    2.47.
## Up to batch 4, the average loss is    2.46.
## Up to batch 5, the average loss is    2.54.
## Up to batch 6, the average loss is    2.48.
## Up to batch 7, the average loss is    2.39.
## Up to batch 8, the average loss is    2.60.
## Up to batch 9, the average loss is    2.62.
## Up to batch 10, the average loss is    2.59.
## Up to batch 11, the average loss is    2.61.
## Up to batch 12, the average loss is    2.62.
## Up to batch 13, the average loss is    2.60.
## Up to batch 14, the average loss is    2.64.
## Up to batch 15, the average loss is    2.62.
## Up to batch 16, the average loss is    2.64.
## The average loss for epoch 12 is      2.64 and mean absolute error is    1.24.
##
## Epoch 013: Learning rate is 0.0010.
## Up to batch 1, the average loss is    2.82.
## Up to batch 2, the average loss is    2.46.
## Up to batch 3, the average loss is    2.42.
## Up to batch 4, the average loss is    2.42.
## Up to batch 5, the average loss is    2.50.
## Up to batch 6, the average loss is    2.45.
## Up to batch 7, the average loss is    2.36.
## Up to batch 8, the average loss is    2.57.
## Up to batch 9, the average loss is    2.59.
## Up to batch 10, the average loss is    2.57.
## Up to batch 11, the average loss is    2.59.
## Up to batch 12, the average loss is    2.60.
## Up to batch 13, the average loss is    2.59.
## Up to batch 14, the average loss is    2.62.
## Up to batch 15, the average loss is    2.61.
## Up to batch 16, the average loss is    2.63.
## The average loss for epoch 13 is      2.63 and mean absolute error is    1.23.
##
## Epoch 014: Learning rate is 0.0010.
## Up to batch 1, the average loss is    2.79.
## Up to batch 2, the average loss is    2.44.
## Up to batch 3, the average loss is    2.40.
## Up to batch 4, the average loss is    2.41.
## Up to batch 5, the average loss is    2.49.
## Up to batch 6, the average loss is    2.44.
## Up to batch 7, the average loss is    2.34.
## Up to batch 8, the average loss is    2.56.
## Up to batch 9, the average loss is    2.58.
## Up to batch 10, the average loss is    2.56.
## Up to batch 11, the average loss is    2.58.
## Up to batch 12, the average loss is    2.59.
## Up to batch 13, the average loss is    2.58.
## Up to batch 14, the average loss is    2.61.
## Up to batch 15, the average loss is    2.60.
## Up to batch 16, the average loss is    2.62.
## The average loss for epoch 14 is      2.62 and mean absolute error is    1.23.</code></pre>
</div>
<div id="built-in-keras-callbacks" class="section level3">
<h3>Built-in Keras callbacks</h3>
<p>Be sure to check out the existing Keras callbacks by reading the <a href="https://keras.posit.co/reference/index.html#callbacks">API
docs</a>. Applications include logging to CSV, saving the model,
visualizing metrics in TensorBoard, and a lot more!</p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
